{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional GANs\n",
    "**활성화**는 임의의 실수를 입력으로 받아 **비선형 미분가능 함수**를 사용하여 특정 범위의 숫자를 출력하는 함수입니다. 우리는 일반적으로 심층 신경망에서, 보다 구체적으로 GAN에서, 특정 층 간에서 분류를 목적으로 사용합니다. 이 비디오에서는 활성화가 무엇인지, 왜 비선형이면서, 왜 미분할 수 있어야 하는지에 대해 설명할 것입니다.\n",
    "\n",
    "* 활성화(activation)란?\n",
    "* 비선형(non-linear) 미분 활성화에 대한 추론\n",
    "\n",
    "### 활성화 함수(Activations) (기본 성질)\n",
    "\n",
    "두 개의 은닉층과 다중 입력이 있는 이 신경망을 살펴보겠습니다. 예를 들어, $x_0$은 털 색상, $x_1$은 동물의 크기 및 나머지는 그 외의 모든 종류의 다른 특징일 수 있으며, 이 신경망은 이러한 특징들이 고양이에 해당하는지 여부를 예측합니다. 이것은 여기서 끝에서 0과 1 사이의 확률을 출력할 것입니다. 그 사이에 있는 이 모든 노드는 어떻습니까? 이 모든 노드가 함께 전체 신경망 아키텍처를 구성하고 이러한 개별 노드 각각에서 어떤 일이 발생하는지 시연하는 것으로 시작하겠습니다. \n",
    "<img src=\"./images/C1W2.01.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "여기 한 노드는 이전 레이어에서 정보를 가져와서 두 가지를 예측합니다. 이 점선으로 나눌 것입니다. \n",
    "먼저 $z$를 계산합니다. 여기서 $i$는 노드가 어떤 노드인지 \n",
    "나타냅니다. 여기서는 $0, 1$이고 첫 번째 노드이므로 $i$는 1이고 $l$은 레이어를 나타냅니다. 여기 0번째 레이어가 있고 여기가 첫 번째 레이어에 있습니다. 다시 말하지만, $l$은 1입니다. \n",
    "여기서 $z$는 이전 계층의 출력에 대한 다양한 가중치의 합과 같습니다. \n",
    "여기에서 $a$ 는 이전 레이어에서 이러한 노드의 다른 쪽이 들어오는 것이며, \n",
    "$[l-1]$은 이전 레이어임을 의미합니다. 여기가 0-번째 레이어입니다. 그것은 모든 다른 값을 살펴봅니다. 여기서 $i$는 0 부터 여기까지 $i$는 $n$과 같습니다. $z$는 이전 레이어를 보고 가중치를 부여합니다. 즉, $z$는 이러한 값 $W$에 의해 가중치가 부여된 이전 레이어의 출력이며 일반적으로 선형 레이어라고 합니다. 둘 다 가중치의 스케일링 값과 여기에 집어넣을 수 있는 편향 항을 포함하며, 이는 해당 값을 이동시킵니다. 여기에서 더하기 $b$를 말하는 것과 같지만 일반적으로 이를 행렬 $W$에 넣을 수 있습니다. 그러면 다른 쪽에는 $a$가 있고 $a$는 여기에서 $g()$라고 하는 활성화 함수의 출력으로 이 $z$를 입력으로 사용합니다. 여기에서 미분 가능하고 비선형적인 의미가 무엇인지 빠르게 살펴보겠습니다. \n",
    "<img src=\"./images/C1W2.02.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "미분 가능하고 비선형인 활성화 함수 $g()$가 필요합니다. 신경망을 훈련하고 매개변수를 업데이트하기 위해 역전파를 사용하기 때문에 미분 가능해야 합니다. 이전 층에 대하여 미분가능하고 그라디언트를 제공할 수 있어야 합니다. 또한 비선형이어야 합니다. 신경망 내에서 계산하는 특징은 복잡할 수 있습니다. 비선형 활성화를 사용하지 않았다면 다중 은닉층과 뉴런이 있는 이와 같은 신경망은 실제로 단순한 선형 회귀로 축소될 수 있습니다. 이러한 행 입력을 여기에서 $x$로 사용하면 이 두 레이어가 실제로 단일 레이어가 될 수 있습니다. 선형 레이어는 서로 쌓을 수 있기 때문에 비선형성이 없습니다. 선형 회귀는 편향 항이 가중치 행렬에 다시 삽입된 하나의 선형 레이어일 뿐입니다.\n",
    "<img src=\"./images/C1W2.03.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "### 요약\n",
    "요약하면, 딥 러닝 모델을 활용하고 복잡한 비선형 함수를 모델링하기 위해 복잡한 신경망을 구성하기 위해 레이어 스택을 서로 겹치게 하려면 비선형 미분 가능한 활성화가 필요합니다. 비선형성은 특히 선형 회귀가 선형 회귀에서 단일 선형 층으로 축소되지 않도록 하므로 네트워크가 더 복잡한 함수을 학습할 수 있고,\n",
    "미분가능성 측면은 네트워크가 미분을 계산하여 역전파를 통해 학습할 수 있도록 합니다. 비선형이고 미분 가능한 한 딥 러닝 모델의 활성화로 모든 사용자 지정 함수를 사용할 수 있습니다. 연구자들은 종종 가장 잘 작동하는 것을 실험하고, 다음 강의에서 일반적으로 사용되는 활성화 함수에 대해 배웁니다.\n",
    "\n",
    "* 활성화 함수는 비선형(non-linear)이며 미분가능(differentiable)해야함\n",
    "* 미분가능(differentiable)은 역전파(backpropagation)을 위함이다.\n",
    "* 비선형(non-linear)은 복잡한 함수를 근사하기 위함이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Activation Functions\n",
    "활성화 함수가 딥 러닝 모델에 중요하다는 것을 보았습니다. 활성화 함수로 사용되는 여러 함수가 있습니다. 이 비디오에서는 오늘날 가장 많이 사용되는 몇 가지를 볼 수 있습니다. 이 비디오는 GAN 에서 사용할 일반적으로 사용되는 4가지 활성화 함수에 초점을 맞춥니다. 첫 번째는 ReLU, 두 번째는 Leaky ReLU라는 변종, 마지막 두 가지는 Sigmoid와 Tanh입니다. 실제로 가능한 활성화 함수의 수는 무한하지만 모든 것이 이상적인 것은 아닙니다.\n",
    "\n",
    "* ReLU\n",
    "* Leaky ReLU\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "\n",
    "#### Activations: ReLU\n",
    "가장 널리 사용되는 효과적인 활성화 함수 중 하나는 Rectified Linear Unit 또는 줄여서 ReLU로 알려져 있습니다. ReLU가 하는 일은 $z$와 0 사이의 최대값을 취하는 것입니다. 즉, 입력이 이 현재 층 $l$의 $z$이면 이 활성화 $g$(여기서는 ReLU가 $g$)는 값 0과 $z$ 사이에서 최대값을 취합니다. 그것이 의미하는 바는 그것이 모든 음수는 버린다는 것입니다. 그래픽으로, 양수 값에 대해 기울기가 1인 직선처럼 보이도록 함수를 설명할 수 있습니다. 값 2와 같이 $z$에 들어오는 모든 값은 $g$에 들어가면 여전히 값 2입니다. 이제 $z$의 음수가 $g$에 전달되면 그래픽으로 해당 값 $z$가 음수일 때마다 0이 출력됩니다. 기본적으로 음수 값은 허용되지 않으므로 비선형으로 만드는 하키 스틱과 유사하게 보입니다. 선형은 단일 직선을 의미합니다. ReLU가 엄밀히 말하면 $z$가 0일 때 미분할 수 없다는 것을 알 수 있습니다. 그러나 관례와 구현에 따라 $z$가 0일 때에 ReLU의 미분은 종종 0으로 설정됩니다. 괜찮습니다. $z$가 음수일 때 ReLU 활성화 함수의 평평한 부분은 항상 0과 같은 미분을 갖습니다. 학습 프로세스가 가중치를 업데이트하는 방법에 대한 중요한 정보를 제공하는 미분에 의존하기 때문에 문제가 될 수 있습니다. 0 미분을 사용하면 일부 노드가 동일한 값에 고정되고 가중치가 학습을 중지합니다. 네트워크의 해당 부분은 학습을 중지합니다. 실제로 네트워크의 이전 구성 요소도 모두 영향을 받습니다. 이 문제는 학습의 끝이기 때문에 죽어가는 ReLU 문제로 알려져 있으며 이것이 ReLU의 변형이 존재하는 이유입니다. Leaky ReLU라고 합니다.\n",
    "<img src=\"./images/C1W2.04.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### Activations: Leaky ReLU\n",
    "Leaky ReLU가 하는 일은 ReLU와 같은 형태를 유지하고, $z$가 양수인 경우에도 ReLU와 같은 형태를 유지한다는 것인데, 이는 입력이 무엇이든 간에 다시 같은 양의 값을 유지하지만 $z$가 0보다 작을 때 선에 약간의 누출 또는 기울기, $z$가 여기에서 음수이고 $z$에서 기울기의 굽힘이 0과 같은 비선형이지만 이제 $z$가 0이 아닌 미분을 갖습니다. 이 기울기는 1보다 작아야 하므로 이 양수의 것과 선을 형성할 필요가 없습니다. 그것은 불행하게도 선형으로 만들 것입니다. $z$에서 미분이 0과 같으면 여전히 0으로 설정됩니다. 일반적으로 기울기는 하이퍼파라미터로 처리되며 여기에 해당하지만 일반적으로 0.1로 설정됩니다. 즉, 여전히 양의 기울기에 비해 누출이 상당히 작습니다. 이것은 확대하여 죽어가는 ReLU 문제를 해결합니다. 실제로 대부분의 사람들은 여전히 ReLU를 사용하지만 Leaky ReLU가 인기를 따라잡고 있습니다.\n",
    "<img src=\"./images/C1W2.05.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### Activations: Sigmoic\n",
    "이제 서로 상당히 유사한 두 가지 다른 일반적인 활성화 함수을 보여 드리겠습니다. 첫째, 매끄러운 모양을 갖고 0-1 사이의 값을 출력하는 시그모이드 활성화입니다. $z$가 0보다 크거나 같으면 시그모이드 활성화가 0.5와 1 사이의 값을 출력합니다. $z$가 0보다 작으면 시그모이드가 0과 0.5 사이의 값을 출력합니다. 0과 1 사이의 값을 출력하기 때문에 sigmoid 활성화 함수는 마지막 계층의 이진 분류 모델에서 종종 0과 1 사이의 확률을 나타내는 데 사용됩니다. 예를 들어, 사진에 고양이가 있다고 예측되면, 0.95의 확률로 나타날 수 있습니다. 요즘에는 시그모이드 활성화 함수는 함수의 미분이 이 함수의 꼬리에서 0에 접근하기 때문에 은닉층에서 자주 사용되지 않습니다. 이것은 그라디언트 소실 문제 또는 함수의 꼬리에서 포화된 출력이라고 부르는 것을 생성합니다. 이 함수는 모든 실제 값을 입력으로 사용할 수 있기 때문에 양방향으로 계속 진행된다고 상상할 수 있습니다. 이것은 위쪽에서 1에 점근적으로 접근하고, 아래쪽에서 0에 점근적으로 접근합니다. 이것은 여러분이 여기 꼬리 부분에 포화된 출력을 가지고 있고, 입력 z가 0에서 너무 멀어지면 값이 항상 1에 가까우거나 0에 가까울 것이기 때문에 그라디언트 소실 문제라고 부르는 것이 나타납니다.\n",
    "<img src=\"./images/C1W2.06.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### Activations: Tanh\n",
    "시그모이드와 모양이 비슷한 또 다른 함수는 쌍곡선 탄젠트(hyperbolic Tangent) 또는 줄여서 tanh입니다. 그러나 시그모이드와 달리 -1 과 1 사이의 값을 출력합니다. z가 양수이면 0 과 1 사이의 값을 출력하고, z가 음수이면 -1 과 0 사이의 음수 값을 출력합니다. 시그모이드와의 한 가지 주요 차이점은 tanh는 입력 z의 부호를 유지하므로 음수는 여전히 음수입니다. 일부 응용 프로그램에서 유용할 수 있습니다. 그러나 모양이 Sigmoid와 유사하기 때문에 동일한 포화 및 그라디언트 소실 문제가 발생합니다. 다시 말하지만, 꼬리는 위쪽에 있는 다리와 아래쪽에 있는 다리의 양쪽으로 확장됩니다. 둘 다 신경망에서 사용되며, 사실 이러한 모든 활성화 기능은 신경망, 특히 곧 구현하게 될 GAN 에서 사용됩니다. 더 많은 활성화가 있고 새로운 활성화가 항상 연구원에 의해 개발 과정에 있습니다. 관심이 있다면 고유한 활성화 함수를 생각해 낼 수 있습니다. 비선형이고 미분 가능한지 확인하십시오.\n",
    "<img src=\"./images/C1W2.07.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 요약\n",
    "요약하자면, 현재 많은 다른 함수가 활성화 함수으로 사용됩니다. 나는 ReLU, Leaky ReLU, sigmoid 및 tanh를 보여줬고 대부분은 자체 문제가 있습니다. 죽어가는 ReLU 문제가 있는 ReLU, 포화 문제에서 소실 성분이 있는 Sigmoid 및 Tanh. 전문화 과정 전반에 걸쳐 구현할 모델에 대해 여기에 제공된 이러한 모든 활성화를 보고 사용할 수 있습니다.\n",
    "\n",
    "* ReLU 활성화는 죽어가는 ReLU 의 영향을 받는다.\n",
    "* Leaky ReLU 는  죽어가는 ReLU 문제를 해결한다.\n",
    "* Sigmoid 와 Tanh는 소멸하는 그라디언트와 saturation 영향을 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 정규화(Batch Normalization) (설명)\n",
    "\n",
    "GAN은 훈련하는 데 많은 시간이 소요됩니다. 특히 이 과정에서 논의하는 것과 같은 정말 멋진 애플리케이션을 위해 GAN을 구축하려는 경우에는 더욱 그렇습니다. GAN은 분류기만큼 간단하지 않기 때문에 학습할 때 종종 매우 취약합니다. 그리고 때때로 생성기와 판별기의 기술이 예측 처럼 정렬되지 않습니다. 이러한 이유로 안정된 훈련에서 속도를 높이는 모든 트릭은 이러한 모델에 중요하며 배치 정규화는 이러한 목적에 매우 효과적인 것으로 입증되었습니다. 시작하기 위해 정규화가 훈련에 미치는 영향을 상기시켜 드리겠습니다. 내부 공변량 이동이 무엇을 의미하고 왜 발생하는지 보여 드리겠습니다. 그리고 마지막에는 배치 정규화가 신경망에서 훈련 속도를 높이고 안정화하는 이유에 대한 직관에 연결될 것입니다.\n",
    "\n",
    "* 정규화는 어떻게 모델을 돕는가?\n",
    "* 내부 공변량 이동(covariate shift)\n",
    "* 배치 정규화(batch notmalization)\n",
    "\n",
    "#### 서로 다른 분포\n",
    "따라서 두 개의 입력 변수, 크기 $x_1$과 선호하는 색상 $x_2$를 갖는 매우 간단한 신경망이 있다고 가정합니다. \n",
    "이러한 특징을 기반으로 하는 이 예제가 고양이인지 여부를 출력하는 단일 활성화를 가집니다. \n",
    "<img src=\"./images/C1W2.08.png\" width=\"700\">\n",
    "<br>\n",
    "그리고 데이터 세트에 대한 $x_1$의 분포(여기서 크기)는 중간 규모의 예를 중심으로 이와 같이 정상적으로 분포되어 있다고 가정해 보겠습니다. 예를 들어 극소수의 매우 작거나, 매우 큰 예가 있다고볼 수 있습니다. 반면에 털 색상 $x_2$의 분포는 훨씬 더 큰 평균과 더 작은 표준 편차로 이처럼 약간 큰 값쪽으로 치우친 걸 가정합니다. 여기에서 더 큰 값은 더 어두운 모피 색상을 나타냅니다. 그리고 물론 털 색상과 크기에 대한 동일한 값을 비교하는 것은 사과와 오렌지를 비교하는 것과 약간 비슷합니다. 어두운 털 색상과 큰 크기는 실제로 의미 있는 상관 관계가 없기 때문입니다. 그러나 이와 같이 서로 다른 분포를 갖는 결과는 신경망 학습에 영향을 미칩니다. \n",
    "<img src=\"./images/C1W2.08-1.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "예를 들어 여기에서 이 로컬 최소값에 도달하려고 하고, 입력 간에 매우 다른 분포가 있는 경우, 이 비용 함수가 길어집니다. 따라서 각 입력과 관련된 가중치의 변경은 이 비용 함수에 다양한 영향을 미치는 다양한 효과를 갖습니다. 그리고 이것은 훈련을 상당히 어렵게 만들고, 가중치를 초기화하는 방법에 따라 더 느리고 크게 의존하게 만듭니다.\n",
    "<img src=\"./images/C1W2.09.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "또한 새로운 훈련 또는 테스트 데이터가 색상에 대해 실제로 밝으므로 상태가 분포 종류의 이동 또는 어떤 식으로든 변경되는 경우 비용 함수의 형태도 변경될 수 있습니다. 그래서 지금 여기가 조금 더 둥글고 최소값의 위치도 이동할 수 있음을 보여줍니다. 어떤 것이 고양이인지 아닌지를 결정짓는 사실이 정확히 동일하게 유지되더라도. 그것이 고양이인지 아닌지에 대한 이미지의 레이블이 변경되지 않은 것입니다. 그리고 이것을 공변량 이동이라고 합니다. 그리고 이것은 데이터 분포가 어떻게 이동하는지에 대한 예방 조치가 취해지지 않은 훈련과 테스트 세트 사이에서 꽤 자주 발생합니다.\n",
    "<img src=\"./images/C1W2.10.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 정규화와 그 효과\n",
    "그러나 입력 변수 $x_1$과 $x_2$가 정규화되면 어떻게 될까요? 정규화가 의미하는 것은,  즉, 새로운 입력 변수 $x_1'$ 및 $x_2'$ 의 분포는 평균이 0이고 표준 편차가 1인 경우 처럼 되는 겁니다. 그러면 비용 함수도 이 두 가지에서 더 매끄럽고 균형 잡힌 것처럼 보일 것입니다. 결과적으로 훈련은 실제로 훨씬 쉽고 잠재적으로 훨씬 빠를 것입니다.\n",
    "<img src=\"./images/C1W2.11.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "또한, 예를 들어 훈련부터 테스트에 이르는, 원시 입력 변수의 분포가 얼마나 많이 변경되더라도 정규화된 변수의 평균과 표준 편차는 동일한 위치로 정규화됩니다. 따라서 평균은 0이고 표준 편차는 1입니다. 그리고 훈련 데이터의 경우 배치 통계를 사용하여 수행됩니다. 따라서 각 배치를 훈련할 때 평균과 표준 편차를 취하고 이를 약 0 과 1로 이동합니다. 그리고 테스트 데이터의 경우, 훈련 세트를 거치면서 수집된 통계량을 실제로 살펴보고 이를 사용하여 테스트 데이터를 훈련 데이터에 더 가깝게 분포하게 할 수 있습니다.\n",
    "\n",
    "그리고 정규화를 사용하면 이 공변량 이동의 효과가 크게 감소합니다. 그리고 이것이 입력 변수의 정규화의 주요 효과이며, 공변량 이동을 줄이는 데 있어 해당 비용 함수를 매끄럽게 합니다. 그러나 데이터 세트의 분포가 모델링 작업과 유사하다는 것만 확인하면 공변량 이동은 문제가 되지 않습니다. 따라서 테스트 세트는 분포하는 측면에서 훈련 세트와 유사하게 됩니다.\n",
    "<img src=\"./images/C1W2.12.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "\n",
    "#### 내부 공변량 이동\n",
    "그렇긴 하지만, 제가 여기서 보여드리는 것과 같은 신경망은 실제로 내부 공변량 이동(internal covariate shift)이라고 불리는 것에 취약합니다. 즉, 내부 숨겨진 층의 공변량 이동이라는 것을 의미합니다.\n",
    "<img src=\"./images/C1W2.13-0.PNG\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "따라서 신경망의 첫 번째 은닉층의 활성화 출력을 가져와 바로 여기 이 노드를 보십시오. 모델을 훈련할 때 활성화 값에 영향을 미치는 모든 가중치가 업데이트됩니다. 따라서 이러한 모든 가중치가 업데이트됩니다. 결과적으로 그 활성화에 포함된 값들의 분포는 훈련 과정에 대한 우리의 영향력에 변화를 줍니다. 이것은 털 색상과 같은 입력 변수 분포 변화에서 본 변화와 유사한 변화로 인해 훈련 과정을 어렵게 만듭니다. 털 색상과 크기보다 조금 더 추상적인 의미를 갖는 내부 노드에서만.\n",
    "<img src=\"./images/C1W2.13.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 배치 정규화\n",
    "이제 배치 정규화는 상황을 해결하려고 합니다. 그리고 각 입력 배치에 대해 계산된 통계를 기반으로 이러한 모든 내부 노드를 정규화합니다. 그리고 이것은 내부 공변량 이동을 줄이기 위한 것입니다. 그리고 이것은 비용 함수를 매끄럽게 하고 신경망을 훈련하기 쉽게 만들고 전체 훈련 과정의 속도를 높이는 추가적인 이점이 있습니다. 따라서 배치 정규화의 배치라는 단어는 배치 통계의 사용을 나타냅니다. 이번 주 비디오를 계속하면서 더 익숙해질 것입니다.\n",
    "<img src=\"./images/C1W2.14.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 요약\n",
    "요약하면 배치 정규화는 비용 함수를 부드럽게 하고, 내부 공변량 이동의 영향을 줄입니다. 훈련 속도를 높이고 안정화하는 데 사용되며 훌륭한 GAN을 구축할 때 유용합니다.\n",
    "\n",
    "* 배치 정규화는 **비용함수를 부드럽게** 해준다.\n",
    "* 배치 정규화는 **내부 공변량 이동을 감소**시킨다.\n",
    "* 배치 정규화는 **학습의 속도를 높여준다**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 정규화 (과정)\n",
    "배치 정규화는 어렵게 들릴 수 있지만, 처음부터 구현할 수 있는 매우 간단한 절차입니다. 그러나 PyTorch와 같은 프레임워크는 실제로 이를 수행합니다. 이 비디오에서는 작동 방식을 보여 드리겠습니다. 이 비디오에서 배치 정규화 작업과 함께, 훈련과 테스트 중에 이러한 정규화 작업이 어떻게 다른지 알게 될 것입니다. 배치 정규화는 정규화된 분포를 생성한다고 이해하시면 됩니다.\n",
    "\n",
    "* 훈련을 위한 배치 정규화\n",
    "* 시험를 위한 배치 정규화\n",
    "\n",
    "#### 배치 정규화: 훈련\n",
    "두 개의 은닉층, 다중 입력 및 단일 출력이 있는 이 신경망을 살펴보겠습니다. 배치 정규화가 어떻게 작동하는지 설명하기 위해 이 내부 은닉 계층에 초점을 맞출 것입니다. 입력에서 $z$는 모든 이전 노드에서 가져옵니다. 배치 정규화에서는 배치의 모든 예제 $z_i$를 고려합니다. 예를 들어 배치 크기가 32일 수 있으므로 여기에는 32개의 $z$가 있습니다. 다시 말하지만, 여기서 $i$는 이것이 이 계층의 $i$번째 노드임을 나타내고 $l$은 이것이 $l$번째 계층임을 나타냅니다. 예를 들어, 여기에 레이어 $0$, 레이어 $1$이 있고, 노드 $0$, 노드 $1$이 있습니다. 배치 정규화는 배치의 크기를 취합니다(예: 32). 여기에는 32개의 $z$가 있습니다. 이 32개의 $z$에서 평균이 0이고 표준 편차가 1이 되도록 정규화하려고 합니다. 당신이 하는 일은 여기에서 배치의 평균 $\\mu$를 구하는 것입니다. 이것은 이 32개의 모든 값에 대한 평균일 뿐입니다. 그런 다음 이 32개 값으로부터 배치의 분산 $\\sigma^2$도 얻습니다. 이 $z$ 값을 0의 평균과 1의 표준 편차로 정규화하려면 평균을 빼고 표준 편차로 나눕니다. 여기에서 분산의 제곱근이고, 분모가 0이 되지 않도록 여기에 $\\epsilon$을 추가합니다. 결국 $\\hat{z}$이라고 하는 이러한 정규화된 $z$ 값을 얻습니다.\n",
    "\n",
    "<img src=\"./images/C1W2.15.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "따라서, 정규화 값 $\\hat{z}$을 얻은 후 배치 정규화 층에서 매개변수를 학습합니다. 이것이 의미하는 바는 이동 계수가 될 $\\beta$와 축척 계수가 될 $\\gamma$라는 값이 있다는 것입니다. 이러한 매개변수는 훈련 중에 학습되어, $z$를 변환하는 분포가 작업에 최적인지 확인합니다. 모든 것을 $\\hat{z}$으로 완전히 정규화한 후 이러한 학습된 값인 $\\gamma$ 및 $\\beta$를 기반으로 스케일을 재조정합니다. 이것이 입력 정규화와 배치 정규화의 주요 차이점입니다. 여기에서는 분포가 매번 0의 평균과 1의 표준 편차를 갖도록 강요하지 않기 때문에, 정규화한 후에 필요한 작업으로 계속해서 재조정할 수 있습니다. 여기서 핵심은 배치 정규화를 통해 해당 분포가 신경망에서 앞으로 이동하는 모습을 제어할 수 있으며, 이동 및 크기 조정 후의 이 최종 값을 $y$라고 합니다. 이 $y$가 이 활성화 함수로 들어가는 것입니다.\n",
    "<img src=\"./images/C1W2.16.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 배치 정규화: 시험\n",
    "이제 그것은 훈련 중이었고, 테스트 중에는 다른 배치의 다른 평균과 표준 편차를 얻어 사용되는 것을 방지하려고 합니다. 이는 동일한 자료들에도, 다른 배치에서는 특정 배치 평균 또는 특정 배치 표준 편차를 사용하여, 다르게 정규화되어 다른 결과를 줄 수 있기 때문입니다. 대신에 우리는 테스트 시에도 안정적인 예측을 원하게 됩니다. 테스트 중에 정규화에 사용하는 것은 전체 훈련 세트에 대해 계산된 실행 평균과 표준 편차이며, 이러한 값은 이제 훈련 후에 고정되어 움직이지 않습니다. 그러나 그 후에는 매우 유사한 프로세스를 따릅니다. 여기 그 $z$ 값의 예상 값이 있습니다. 그래서 그것은 실행 평균이고 여기에 $z$ 값의 분산이 있습니다. 여기에서 제곱근을 취하면 표준 편차를 얻을 수 있습니다. 분모가 0이 되는 것을 방지하기 위해 여전히 $\\epsilon$이 있습니다. 그 후에는 훈련에서와 동일한 프로세스를 따르고 이러한 정규화된 값을 학습 매개변수에 입력한 다음 활성화 함수에 입력합니다. 그러나 이 프로세스의 세부 사항에 대해 너무 걱정하지 마십시오. 전에 말했듯이 TensorFlow 및 PyTorch와 같은 프레임워크는 이러한 통계를 추적합니다. BatchNorm 이라는 레이어를 생성하기만 하면 됩니다. 그런 다음 모델을 테스트 모드로 전환하면 실행 중인 통계가 전체 데이터 세트에 대해 계산되거나 저장되므로 정말 좋습니다. 테스트 모드를 테스트 시간, 추론 시간, 평가 또는 평가 모드라고 하는 것을 들어본 적이 있을 것입니다. 이것들은 거의 동등하며, 훈련 시간이 아니라고 생각할 수 있습니다.\n",
    "<img src=\"./images/C1W2.17.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 요약\n",
    "요약하면, 배치 정규화는 표준 정규화와 다릅니다. 훈련하는 동안 전체 데이터 세트가 아니라 각 배치의 통계를 사용하기 때문입니다. 이렇게 하면 계산 시간이 줄어들고, 사용자가 배치 정규화하기에 앞서서 전체 데이터 세트가 먼저 처리되기를 기다리지 않기 때문에 훈련 속도가 빨라집니다. 둘째, 학습 가능한 이동 및 축척 매개변수를 도입하여 목표 분포가 0의 평균과 1의 표준 편차를 갖도록 강요하지 않습니다. 마지막으로 테스트 시에는 훈련의 실행 통계가 전체 데이터 세트에 적용되어 훈련 값이 독립적이고 고정되어 있기 때문에 예측을 안정적으로 유지합니다. 한 가지 중요한 점은 프레임워크가 훈련 및 테스트를 위해 이 전체 프로세스를 구현한다는 것입니다. 당신이 해야 할 일은 훈련하려는 다른 멋진 모델에서 언제 사용해야 하는지만 아는 것입니다. 또한 앞으로 몇 주 동안 고급 정규화 기술에 대해 배우게 되므로 계속 지켜봐 주시기 바랍니다.\n",
    "\n",
    "* 배치 정규화는 학습 가능한 이동 및 축척 요소을 도입합니다.\n",
    "* 시험하는 동안, 훈련시 얻어진 통계치가 사용된다.\n",
    "* 프레임워크가 이 모든 과정을 담당한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨볼루션 복습\n",
    "컨볼루션은 이미지 처리의 핵심 부분에 있으므로 많은 GAN 아키텍처에서 특히 중요한 부분입니다. 시작하기 위해 정말 간단한 예를 사용하여 컨볼루션이 무엇이고 어떻게 작동하는지 살펴보겠습니다.\n",
    "\n",
    "* 컨볼루션이란?\n",
    "* 어떻게 동작하는가?\n",
    "\n",
    "#### 컨볼루션이란?\n",
    "Convolution을 사용하면 필터를 사용하여 이 귀여운 강아지 사진과 같은 이미지의 모든 영역에서 주요 특징을 감지할 수 있습니다. 이 필터는 정의된 다양한 특징을 이미지 전체에서 스캔합니다. 예를 들어, 아이 필터는 이미지의 어느 부분에 눈이 있는지, 코 필터는 어느 부분에 코가 있는지, 귀 필터는 어느 부분에 귀가 있는지 알려줍니다. 이러한 각 필터는 실수 값의 행렬일 뿐이며, 이러한 정확한 값은 훈련 중에 학습됩니다. 이 값은 입력 이미지에 대한 컨볼루션을 계산하는 데 사용됩니다. 따라서 실제로는 눈이나 코 필터보다 훨씬 더 추상적이지만 눈과 코와 같은 상당히 높은 수준의 특징을 선택합니다.\n",
    "\n",
    "<img src=\"./images/C1W2.18.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "컨볼루션 연산은 매우 간단합니다. 여기 각 정사각형이 0에서 255 사이의 값을 갖는 픽셀인 이 5x5 그레이스케일 이미지를 살펴보세요. 여기서 0은 완전히 검은색 픽셀을 나타내고 255는 완전히 흰색 픽셀을 나타냅니다. 따라서 회색 값은 여기에서 볼 수 있는 50과 같이 그들 사이에 있게됩니다. 예를 들어 첫 번째 열에 1, 두 번째 열에 0, 마지막 세 번째 열에 -1이 있는 학습된 3x3 필터가 있다고 가정해 보겠습니다. 회색조 이미지에서 이 필터를 사용하는 컨볼루션 연산은 그 사이에 별표로 표시하겠습니다. 이것은 이 회색조 이미지에 이 필터를 적용하려는 것을 의미합니다. \n",
    "<img src=\"./images/C1W2.20-0.PNG\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "여기서 보이는 것은 왼쪽 상단 모서리에서 시작하여 해당 픽셀 위에 필터의 모든 요소를 곱하는 것입니다. 50에 1을 곱하고 50에 0을 곱하고 50에 음수를 곱하는 식으로 계속해서 필터와 매칭되는 픽셀과 곱 합니다. 그런 다음 이 모든 곱셈의 합계를 가져와 결과 행렬에 넣습니다. 여기 첫 번째 셀에 입력합니다. \n",
    "<img src=\"./images/C1W2.19.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그런 다음 필터를 오른쪽으로 한 위치 이동하여 요소별 곱을 얻고 결과를 다시 합산하고 해당 값을 행렬 등에 저장하는 식으로 계속됩니다. 다음엔 아래로 한칸 이동합니다. 이제 끝에 도달했으므로 왼쪽에서 시작하여 아래로 내려가 전체 이미지를 필터로 덮을 때까지 필터를 계속 적용합니다. 그런 다음 마지막에 이 3x3 결과를 얻습니다. 흥미로운 점은 필터가 실제로 수직선 감지기라는 것입니다. 여기 이미지에 수직선이 있고 필터가 여기에서 매우 높은 값을 보고 \"여기 수직 가장자리가 정말 보여요\"라고 말하려고 합니다. 이러한 필터는 다양한 값을 가질 수 있으며, 이러한 필터의 여러 레이어를 구성하면 코나 눈과 같은 보다 추상적이고 높은 수준의 개념을 얻을 수 있습니다. 이것이 컨볼루션 계산의 전부입니다. 하지만 이 간단한 작업에 대해 작성할 수 있는 몇 가지 트릭이 있어, 앞으로 나아가는 데 도움이 될 것입니다. 이에 대해서는 다음에 설명하겠습니다.\n",
    "<img src=\"./images/C1W2.20.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 요약\n",
    "요약하자면, 컨볼루션은 이미지 처리에 사용되는 정말 중요한 작업이며 이미지의 각 섹션을 스캔하고 특징을 감지하여 패턴을 인식합니다. 결국, 컨볼루션은 전체 이미지에 걸친 요소별 곱의 일련의 합계일 뿐입니다.\n",
    "\n",
    "* 컨볼루션은 이미지를 처리할때 유용한 층이다.\n",
    "* 유용한 특징을 감지하기위해 이미지를 스캔한다.\n",
    "* 단지 요소별(element-wise) 곱셈과 덧셈으로 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩(padding)과 보폭(stride)\n",
    "컨볼루션은 이미지의 모든 영역에서 특징을 감지하는 데 사용되는 간단한 연산입니다. 이전 비디오에서 단순한 컨볼루션을 이미지에 적용하는 방법을 보여주었습니다. 다음에는 이러한 컨볼루션 작업을 조정하는 몇 가지 방법을 보여 드리겠습니다. 패딩과 보폭이 무엇인지 검토하고, 컨볼루션에 패딩을 사용하는 이면에 있는 몇 가지 직관을 공유하겠습니다.\n",
    "\n",
    "* 패딩과 보폭\n",
    "* 패딩의 직관\n",
    "\n",
    "#### 보폭(Stride)\n",
    "이전 예에서 이 회색조 이미지를 가져옵니다. 3 x 3 컨벌루션 필터를 적용할 때 왼쪽 상단 모서리에 있는 이미지의 처음 3 x 3 섹션을 사용하여 필터의 요소별 곱을 계산하는 것으로 시작합니다. 그런 다음 오른쪽으로 한 픽셀로 이동한 다음 동일한 계산 세트를 수행합니다. 다시 오른쪽으로 픽셀을 이동하고, 다음에는 한 픽셀로 아래로 이동하고, 왼쪽에서 다시 시작하여 끝에 도달할 때까지 이동합니다. 한 픽셀을 오른쪽으로, 한 픽셀 아래로 이동하는 것을 1의 보폭을 사용한다고 합니다. 따라서 매번 한 걸음씩 한 픽셀만 움직입니다. 오른쪽으로 가는 것과 아래로 가는 것의 보폭 값이 같을 필요는 없습니다. 예를 들어, 보폭은 오른쪽으로 2개, 아래로 4개일 수 있습니다. 보폭이 클수록 필터에 적용할 수 있는 이미지가 줄어들지만 동시에 계산 속도가 빨라집니다. 이것은 확실히 여기에서 트레이드 오프입니다.\n",
    "<img src=\"./images/C1W2.21.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "보폭 2는 필터가 여전히 왼쪽 상단 모서리에서 시작되지만 이제는 매번 오른쪽으로 2픽셀씩 이동합니다. 그리고 아래로 2픽셀 아래로 이동하므로 4개의 계산만 수행합니다. 훨씬 빠릅니다. 보다시피 보폭은 필터가 이미지를 방문하는 섹션의 수를 결정합니다. 이 경우 필터를 사용하여 3 x 3 섹션 중 4개를 방문했기 때문에 컨볼루션의 결과는 이제 더 작은 2 x 2 행렬입니다. 상단에 2개 하단에 2개.\n",
    "<img src=\"./images/C1W2.24.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 패딩(Padding)\n",
    "패딩은 기본 컨볼루션에 대한 또 다른 조정이므로 이 3 x 3 이미지와 이 2 x 2 필터를 사용합니다. 여기의 픽셀 값은 중요하지 않으므로 생략했습니다. 이 경우 1의 보폭으로 컨볼루션을 계산하면 이미지의 네 부분을 방문하게 됩니다. 왼쪽 상단 모서리에 하나, 오른쪽 상단 모서리에 하나, 왼쪽 하단에 하나, 오른쪽 하단에 하나. 중앙에 있는 픽셀의 끝에서는 네 번 방문하는 반면 모서리에 있는 픽셀은 한 번만 방문하고 다른 픽셀은 두 번 방문합니다. 따라서 이것은 중앙에 보관된 정보가 일반적으로 가장자리에 있는 정보보다 더 많은 관심을 받는다는 것을 의미합니다. 이것은 당신이 신경 쓰는 특징을 가장자리에 가지고 있는 경우에 문제입니다. 예를 들어 여기 이 픽셀에 있는 전체 개일 수도 있고, 코와 같은 개의 일부일 수도 있고, 여기 위에 있는 어떤 것일 수도 있으므로, 당신은 전체 영상의 모든 부분이 같은 중요도를 갖게 되길 원할 겁니다.\n",
    "<img src=\"./images/C1W2.25.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "이 문제를 해결하기 위해 이미지 주위에 프레임을 배치하여 액자에 넣은 사진처럼 모든 정보가 완전히 새로운 사진의 중앙에 나타나도록 할 수 있습니다. 이것을 패딩이라고 합니다. 패딩 크기는 필터 크기와 이미지 크기에 따라 달라질 수 있습니다. 패딩에 사용되는 값은 실제로 이 외부 값의 어떤 것이든 될 수 있지만 종종 0으로 설정되며 이를 제로 패딩이라고 합니다. 컨볼루션을 계산하기 전에 이러한 작업을 수행하면 필터가 프레임과 함께 이미지를 스캔하지만 이번에는 이미지의 모든 픽셀이 동일한 횟수만큼 방문하게 됩니다. 실제로 방문한 횟수가 적은 픽셀들은 프레임 뿐입니다. 그 이유는 이제 이미지의 모든 픽셀이 해당 프레임으로 둘러싸인 중앙에 있기 때문입니다.\n",
    "<img src=\"./images/C1W2.26.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 요약\n",
    "요약하자면, 보폭은 필터에 이미지를 스캔하는 방법을 알려주고, 출력 크기와 픽셀 방문 빈도에 영향을 줍니다. 패딩은 이미지 가장자리에 있는 픽셀과 중앙에 있는 픽셀에 동일한 중요성을 부여하기 위해, 이미지 주위에 프레임을 두는 것입니다.\n",
    "\n",
    "* 보폭은 필터가 이미지를 어떻게 스캔할 지를 결정한다.\n",
    "* 패딩은 이미지의 프레임과 같다.\n",
    "* 패딩은 가장자리와 중앙에서 유사한 중요도를 갖게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링(pooling)과 업샘플링(upsampling)\n",
    "풀링 및 업샘플링은 컨볼루션을 사용하는 신경망에서의 실제 일반적인 층입니다. 풀링은 입력의 크기를 줄이는 데 사용되는 반면, 업샘플링은 해당 입력의 크기를 늘리는 데 사용됩니다. 풀링이 무엇이며 어떻게 작동하는지 다시 살펴보고, 그 후에 업샘플링이 무엇이며 풀링과 어떤 관련이 있는지 보여드리겠습니다.\n",
    "\n",
    "* 풀링\n",
    "* 업샘플링과 풀링과의 관계\n",
    "\n",
    "#### 풀링\n",
    "풀링은 주어진 각 영역에서의 평균을 취하거나 최대값을 찾아 입력 이미지의 차원을 낮추는 데 사용됩니다. 예를 들어, 풀링 레이어 이후에 이 고양이의 사진은 더 작은 크기 또는 더 낮은 해상도로 이 흐릿한 이미지를 초래할 것입니다. 이 예에서 색상 팔레트와 두 이미지의 색상 분포가 여전히 매우 유사함을 알 수 있습니다. 흐릿한 이미지의 모양은 여전히 원본과 비슷합니다. 이 원본 이미지에서보다 이 풀링 레이어에서 계산을 수행하는 것이 훨씬 저렴합니다. 풀링은 실제로 해당 정보를 추출하는 것이라고 볼 수 있읍니다.\n",
    "<img src=\"./images/C1W2.27.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 최대 풀링(Max Pooling)\n",
    "최대 풀링이라고 하는 가장 인기 있는 풀링 유형 중 하나를 살펴보겠습니다. 다음은 정말 간단한 예와 함께 작동하는 방법입니다. 이 4x4 그레이스케일 이미지를 가져오고 최대 풀링을 사용하여 여기에서 2x2 출력 이미지로 축소한다고 가정합니다. 여러분이 하는 일은 컨볼루션과 정말 유사한 방식으로 이미지의 다른 섹션이나 창을 방문하는 것입니다. 원하는 결과를 얻으려면 어떤 의미에서는 2x2 풀링 필터를 사용해야 하며, 여기서 필터를 말하고 있지만 학습된 가중치가 없습니다. 그것은 통과되는 창일 뿐이며 보폭은 2이므로 픽셀을 두 번 이상 방문하지 않습니다. 이미지의 왼쪽 상단 모서리에서 시작하여 해당 창에서 최대값인 20을 찾습니다. 이 값을 가져 와서 여기에 출력에 넣습니다.\n",
    "<img src=\"./images/C1W2.28.png\" width=\"700\">\n",
    "<br>\n",
    "다음으로 오른쪽 상단 모서리에서 동일한 작업을 수행합니다. 다시 말하지만 어떤 픽셀도 겹치지 않습니다. 여기를 보세요. 가장 큰 값은 45입니다. 여기에 표시합니다. \n",
    "<img src=\"./images/C1W2.29.png\" width=\"700\">\n",
    "<br> \n",
    "그런 다음 전체 이미지를 덮을 때까지 계속해서 입력에서 방문한 2x2 섹션 각각의 최대값을 포함하는 2x2 출력을 얻습니다. 여기에서 최대 풀링이 하는 것은 이 이미지에서 가장 중요한 정보를 얻는 것입니다. 이것은 여기에서 매우 높은 값입니다. 이것은 가장 중요한 정보에만 관심이 있는 정보를 추출하는 데 정말 중요할 수 있습니다. 때때로 이것은 이미지가 아니고, 이것은 단지 중간 레이어이므로, 이것은 실제로 이 중간 섹션에서 가장 높은 값을 가진 가중치를 추출하는 것입니다.\n",
    "<img src=\"./images/C1W2.31.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "최대 풀링 외에 다른 풀링이 있습니다. 평균 풀링이 있으며 해당 창에서 최대값 대신 평균값을 취하는 경우에도 상당히 일반적으로 사용되며, 최소값을 취한 최소 풀링도 있습니다. 다시 말하지만, **풀링에는 학습 가능한 매개변수가 없다**는 것을 아는 것이 정말 중요합니다. 이것이 컨볼루션과 정말 다른 점입니다. 이것들은 학습되지 않습니다. 이미지 전체에 적용되는 간단한 규칙만 있습니다.\n",
    "<img src=\"./images/C1W2.32.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 업샘플링\n",
    "업샘플링은 풀링의 반대 효과를 가집니다. 이 저해상도 이미지를 감안할 때 업샘플링은 더 높은 해상도의 이미지를 출력하는 것을 목표로 합니다. 완벽하지 않다는 것을 알 수 있습니다. 이렇게 하려면 실제로 추가 픽셀에 대한 값을 추론해야 하며 이를 수행하는 몇 가지 다른 방법이 있습니다.\n",
    "<img src=\"./images/C1W2.33.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 업샘플링 : 최근접 이웃(Nearest Neighbor)\n",
    "업샘플링하는 한 가지 쉬운 방법은 최근접 이웃 업샘플링으로 알려져 있으며 이 방법을 사용하면 입력에서 픽셀 값을 여러 번 복사하여 출력을 채울 수 있습니다. 설명을 위해 여기에서 이 4x4 출력으로 업샘플링하려는 2x2 회색조 이미지를 가져옵니다. 먼저 입력의 왼쪽 상단 모서리에 있는 픽셀 값을 출력의 왼쪽 상단 픽셀에 할당합니다. 다른 입력 픽셀들의 경우에는 왼쪽 상단 모서리에서 두 픽셀의 거리를 추가합니다. 다른 경우에 이 거리는 원하는 확대 크기에 따라 다를 수 있습니다. 이번 경우에는 이 대각선을 포함하여 이들 각각 사이에는 정확히 하나의 픽셀이 있습니다. \n",
    "<img src=\"./images/C1W2.34.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그런 다음 가장 가까운 이웃에 있는 것과 동일한 값을 다른 모든 픽셀에 할당합니다. 출력의 모든 2x2 모서리에 대해 픽셀 값이 동일하게 보입니다. 이 값을 모서리에 먼저 넣고 다른 모든 픽셀에 대해 가장 가까운 이웃을 찾는 것으로 생각할 수도 있습니다.\n",
    "<img src=\"./images/C1W2.35.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "다시 말하지만, 업샘플링을 위한 풀링에는 선형(linear) 및 이차-선형(bi-linear) 보간과 같이 업샘플링을 수행하는 다양한 방법이 있습니다. 이들이 하는 일과 구현 방법을 자유롭게 탐색하십시오. 그러나 TensorFlow 및 PyTorch와 같은 프로그래밍 프레임워크가 실제로 이 전체 프로세스를 처리하므로 구현에 대해 걱정할 필요가 없습니다. 입력에서 알려진 값을 보고 출력의 누락된 픽셀에서 값을 유추하고 풀링과 마찬가지로 업샘플링 레이어에는 학습 가능한 매개변수가 없습니다. 그것은 단지 고정된 규칙일 뿐입니다. 이것들은 단지 다른 고정 규칙일 뿐입니다.\n",
    "<img src=\"./images/C1W2.36.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "#### 요약\n",
    "요약하면 풀링 층은 입력 크기를 줄이는 반면, 업샘플링은 그 반대입니다. 컨볼루션과 달리 풀링이나 업샘플링 층에는 학습 가능한 매개변수가 없으며 고정된 규칙만 있습니다. 이제 학습 가능한 매개변수가 있는 전치 컨볼루션으로 알려진 업샘플링 층이 아닌 업샘플링 방법을 소개하겠습니다.\n",
    "\n",
    "* 풀링은 이력의 크기를 줄인다.\n",
    "* 업샘플링은 입력의 크기를 키운다.\n",
    "* 학습할 매개변수가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전치된 컨볼루션(Transposed convolution)\n",
    "이제 컨볼루션, 풀링 및 업샘플링 레이어에 익숙해졌으므로 전치된 컨볼루션에 대해 이야기해 보겠습니다. 이 비디오에서는 학습 가능한 필터를 사용하여 입력 크기를 확대하는 전치된 컨볼루션 및 업샘플링 기술에 대해 설명한 다음, 이 방법의 출력에 픽셀 값 계산 방식과 관련된 어떤 정말 특별한 문제가 있는지 보여 드리겠습니다.\n",
    "\n",
    "* 전치된 컨볼루션은 업샘플링이다.\n",
    "* 전치된 컨볼루션의 문제점들\n",
    "\n",
    "### 전치된 컨볼루션\n",
    "3x3 출력으로 업샘플링하려는 2x2 입력이 있는 이 예를 살펴보겠습니다. 전치된 컨볼루션을 사용하면 이 과정의 앞부분에서 시연한 컨볼루션에 대한 절차와 매우 유사한 절차에 따라 이 작업을 수행하기 위해 보폭이 1인 2x2 학습 필터를 사용할 수 있습니다. 입력에서 왼쪽 상단 값을 가져오고 2x2 필터의 모든 값과 함께 곱셈 결과을 가져오는 것으로 시작합니다. 그런 다음 이 값을 출력의 왼쪽 상단 모서리에 2x2로 저장합니다. 다음은 해당 작업에서 곱한 값입니다.\n",
    "<img src=\"./images/C1W2.37.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "다음으로, 필터를 1보폭으로 이동하고, 이 다음 픽셀에 대해 동일한 프로세스를 반복합니다. 이제 입력의 오른쪽 상단 모서리에 있고 출력에서 이 4개의 픽셀이 있습니다. 거기에 곱셈을 유지하고 겹치는 부분이 있으면 이전 곱셈에 추가합니다.\n",
    "<img src=\"./images/C1W2.38.png\" width=\"700\">\n",
    "<br>\n",
    "그런 다음 입력의 왼쪽 하단 모서리로 이동하여 전체 입력을 덮을 때까지 필터와 함께 곱셈을 가져오고 결과를 보내는 등의 작업을 수행합니다.\n",
    "<img src=\"./images/C1W2.39.png\" width=\"700\">\n",
    "<br>\n",
    "<img src=\"./images/C1W2.40.png\" width=\"700\">\n",
    "<br>\n",
    " 이 계산을 통해 출력의 일부 값이 입력의 값에 훨씬 더 많이 영향을 받는 것을 볼 수 있습니다. 예를 들어, 출력의 중앙 픽셀은 입력의 모든 값의 영향을 받는 반면\n",
    "<img src=\"./images/C1W2.41.png\" width=\"700\">\n",
    "<br>\n",
    " 모서리는 단 하나의 값의 영향을 받습니다. 이것은 전치된 컨볼루션 연산이며 업샘플링에서 하는 모든 것입니다. 필터의 이러한 값들을 학습합니다.\n",
    " 그러나 이 중심 픽셀은 4번 방문하여 모든 픽셀의 영향을 받는 반면 다른 픽셀은 그렇지 않은 문제가 있어 전치된 컨볼루션을 사용할 때 발생하는 일반적인 문제입니다.\n",
    "<img src=\"./images/C1W2.42.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "### 전치된 컨볼루션의 문제\n",
    "이 이미지는 전치된 컨볼루션의 실제 출력을 보여줍니다. 이 원으로 표시된 부분을 확대하면 바둑판 문제가 보이게 됩니다. 이미지에 바둑판 모양의 패턴이 있고 필터로 업샘플링할 때 일부 픽셀은 훨씬 더 크게 영향을 받는 반면 주변 픽셀은 그렇지 않기 때문에 이러한 현상이 발생합니다. 그러나 이 문제에도 불구하고 전치된 컨볼루션은 여전히 연구 커뮤니티에서 상당히 인기가 있지만, 이 체커보드 문제를 피하기 위해 컨볼루션이 뒤따르는 업샘플링을 사용하는 것이 이제 더 인기 있는 기술이 되고 있습니다.\n",
    "<img src=\"./images/C1W2.43.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "\n",
    "#### 요약\n",
    "요약하자면, 전치된 컨볼루션은 업샘플링 방법으로 사용되며 업샘플링 레이어와 달리 학습 가능한 매개변수를 갖습니다. 그러나  전치된 컨볼루션을 사용하여 발생하는 문제는 출력에 바둑판 무늬 문제가 있다는 것입니다. 그럼에도 불구하고 이러한 레이어는 여전히 인기가 많으며 전문 분야의 일부 모델에 사용할 것입니다.\n",
    "\n",
    "* 전치된 컨볼류션은 업샘플링이다.\n",
    "* 이들은 학습할 매개변수를 가진다.\n",
    "* 문제점: 채커보드 패턴을 만든다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
