{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"C3W2A_Assignment-Problem.ipynb","provenance":[],"collapsed_sections":[]},"coursera":{"schema_names":["GANSC3-2A"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1czVdIlqnImH"},"source":["# U-Net"]},{"cell_type":"markdown","metadata":{"id":"1KD3ZgLs80vY"},"source":["### 목표\n","이 노트북에서는 생물의학 이미징 분할화 작업을 위해 U-Net을 구현합니다. 특히, 뉴런에 레이블을 붙일 것이므로 이것을 신경 신경망(neural neural network)이라고 부를 수 있습니다! ;)\n","\n","이것은 GAN, 생성 모델 또는 비지도학습도 아닙니다. 이것은 지도학습 이므로 정답은 하나뿐입니다(예: 분류기!) 이 구성 요소가 이번 주 다음 노트북에서 Pix2Pix의 Generator 구성 요소의 기초가 되는 방법을 볼 수 있습니다.\n","\n","### 학습 목표\n","1. 자신만의 U-Net을 구현합니다.\n","2. 까다로운 분할(segmentation) 작업에서 U-Net의 성능을 관찰하십시오."]},{"cell_type":"markdown","metadata":{"id":"wU8DDM6l9rZb"},"source":["## 시작하기\n","먼저 라이브러리를 가져오고, 시각화 기능을 정의하고, 사용할 신경 데이터 세트를 가져옵니다.\n","\n","#### 데이터세트\n","이 노트북의 경우 전자 현미경 데이터 세트를 사용합니다.\n","이미지 및 분할 데이터. 사용하게 될 데이터세트에 대한 정보는 [여기](https://www.ini.uzh.ch/~acardona/data.html)에서 확인하실 수 있습니다!\n","\n","> Arganda-Carreras et al. \"이미지 생성 크라우드소싱\n","Connectomics를 위한 세분화 알고리즘\". Front. Neuroanat. 2015. https://www.frontiersin.org/articles/10.3389/fnana.2015.00142/full\n","\n","![dataset example](Neuraldatasetexample.png)"]},{"cell_type":"code","metadata":{"id":"R_tC38g7WhTE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633332854828,"user_tz":-540,"elapsed":41411,"user":{"displayName":"(IT융합공학부)지준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIX0Q0mLDcK79IRw8eAKsHxV5Aj7WY08yVbhjGLA=s64","userId":"03314696836681181392"}},"outputId":"09ad2cad-1db6-4a1f-abde-ec897541c46a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# click the link and copy the code and paste it into the box below !"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"z3DhlBoGW6p0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633332947800,"user_tz":-540,"elapsed":293,"user":{"displayName":"(IT융합공학부)지준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIX0Q0mLDcK79IRw8eAKsHxV5Aj7WY08yVbhjGLA=s64","userId":"03314696836681181392"}},"outputId":"07c52607-5d6d-48d8-d707-14bbf97d43be"},"source":["cd drive/MyDrive/Classes/GAN/C3W2/Exercises/ExercisesA/"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Classes/GAN/C3W2/Exercises/ExercisesA\n"]}]},{"cell_type":"code","metadata":{"id":"aixwQ6AZXNqD","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1633332954410,"user_tz":-540,"elapsed":284,"user":{"displayName":"(IT융합공학부)지준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIX0Q0mLDcK79IRw8eAKsHxV5Aj7WY08yVbhjGLA=s64","userId":"03314696836681181392"}},"outputId":"c21b3df0-b2c5-490c-f5ff-d48a4d739437"},"source":["pwd"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Classes/GAN/C3W2/Exercises/ExercisesA'"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"JfkorNJrnmNO"},"source":["import torch\n","from torch import nn\n","from tqdm.auto import tqdm\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","torch.manual_seed(0)\n","\n","def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n","    '''\n","    Function for visualizing images: Given a tensor of images, number of images, and\n","    size per image, plots and prints the images in an uniform grid.\n","    '''\n","    # image_shifted = (image_tensor + 1) / 2\n","    image_shifted = image_tensor\n","    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n","    image_grid = make_grid(image_unflat[:num_images], nrow=4)\n","    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElJ7n7x1itOX"},"source":["## U-Net 아키텍처\n","이제 구성 요소들로부터 U-Net을 구축할 수 있습니다. 아래 그림은 Ronneberger 등의 2015 논문 [*U-Net: Convolutional Networks for Biomedical Image Segmentation*](https://arxiv.org/abs/1505.04597)에서 가져온 것입니다. 그것은 U-Net 아키텍처와 그것이 어떻게 축소되고 확장되는지 보여줍니다.\n","\n","<!-- \"[i]t consists of a contracting path (left side) and an expansive path (right side)\" (Renneberger, 2015) -->\n","\n","![Figure 1 from the paper, U-Net: Convolutional Networks for Biomedical Image Segmentation](https://drive.google.com/uc?export=view&id=1XgJRexE2CmsetRYyTLA7L8dsEwx7aQZY)\n","\n","즉, 이미지는 먼저 높이와 너비를 줄이는 동시에 채널을 늘리는 많은 컨볼루션 레이어를 통해 공급됩니다. 이 레이어를 저자는 \"수축 경로\"라고 부릅니다. 예를 들어, 보폭이 2인 2개의 2 x 2 컨볼루션 세트는 1 x 28 x 28(채널, 높이, 너비) 회색조 이미지를 취하여 2 x 14 x 14 표현을 만들게 됩니다. \"확장 경로\"는 이와 반대로 점점 더 적은 수의 채널로 이미지를 점차적으로 성장시킵니다."]},{"cell_type":"markdown","metadata":{"id":"l3UVfNXvPC15"},"source":["## 수축 경로(contracting path)\n","먼저 수축 경로에 대한 수축 블록을 구현합니다. 이 경로는 U-Net의 인코더 섹션으로, 그 일부로 여러 다운샘플링 단계가 있습니다. 저자는 논문의 다음 단락에서 나머지 부분에 대해 자세히 설명합니다(Renneberger, 2015).\n","\n",">축소 경로는 일반적인 컨볼루션 네트워크 아키텍처를 따릅니다. 이것은 2개의 3 x 3 컨볼루션(패딩되지 않은 컨볼루션)의 반복적인 적용으로 구성되며, 각각은 ReLU 와 다운샘플링을 위해 보폭 2를 사용하는 2 x 2 최대 풀링 작업이 뒤따릅니다. 각 다운샘플링 단계에서 특징 채널 수를 두 배로 늘립니다.\n","\n","<details>\n","<summary>\n","<font size=\"3\" color=\"green\">\n","<b>Optional hints for <code><font size=\"4\">ContractingBlock</font></code></b>\n","</font>\n","</summary>\n","\n","1. 두 컨볼루션 모두 3 x 3 커널을 사용해야 합니다.  \n","2. 최대 풀링은 보폭이 2인 2 x 2 커널을 사용해야 합니다.\n","</details>"]},{"cell_type":"code","metadata":{"id":"xvY4ZNyUviY9"},"source":["# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED CLASS: ContractingBlock\n","class ContractingBlock(nn.Module):\n","    '''\n","    ContractingBlock Class\n","    Performs two convolutions followed by a max pool operation.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels):\n","        super(ContractingBlock, self).__init__()\n","        # You want to double the number of channels in the first convolution\n","        # and keep the same number of channels in the second.\n","        #### START CODE HERE (~4 lines)####\n","\n","        \n","        \n","        \n","        #### END CODE HERE ####\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of ContractingBlock: \n","        Given an image tensor, completes a contracting block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.conv1(x)\n","        x = self.activation(x)\n","        x = self.conv2(x)\n","        x = self.activation(x)\n","        x = self.maxpool(x)\n","        return x\n","    \n","    # Required for grading\n","    def get_self(self):\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F8tXrgXkMdcV"},"source":["#UNIT TEST\n","def test_contracting_block(test_samples=100, test_channels=10, test_size=50):\n","    test_block = ContractingBlock(test_channels)\n","    test_in = torch.randn(test_samples, test_channels, test_size, test_size)\n","    test_out_conv1 = test_block.conv1(test_in)\n","    # Make sure that the first convolution has the right shape\n","    assert tuple(test_out_conv1.shape) == (test_samples, test_channels * 2, test_size - 2, test_size - 2)\n","    # Make sure that the right activation is used\n","    assert torch.all(test_block.activation(test_out_conv1) >= 0)\n","    assert torch.max(test_block.activation(test_out_conv1)) >= 1\n","    test_out_conv2 = test_block.conv2(test_out_conv1)\n","    # Make sure that the second convolution has the right shape\n","    assert tuple(test_out_conv2.shape) == (test_samples, test_channels * 2, test_size - 4, test_size - 4)\n","    test_out = test_block(test_in)\n","    # Make sure that the pooling has the right shape\n","    assert tuple(test_out.shape) == (test_samples, test_channels * 2, test_size // 2 - 2, test_size // 2 - 2)\n","\n","test_contracting_block()\n","test_contracting_block(10, 9, 8)\n","print(\"Success!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONEnbpQcvi_p"},"source":["## 확장 경로\n","다음으로 확장 경로에 대한 확장 블록을 구현합니다. 이것은 부분적으로 몇개의 업샘플링 단계가 있는 U-Net의 디코딩 섹션입니다. 이렇게 하려면 자르기 함수도 작성해야 합니다. 이는 *축소 경로*에서 이미지를 자르고 확장 경로의 현재 이미지에 연결할 수 있도록 하기 위한 것입니다. 이는 건너뛰기 연결을 형성하기 위한 것입니다. 다시 말하지만, 세부 사항은 논문에서 가져온 것입니다(Renneberger, 2015):\n","\n",">확장 경로의 모든 단계는 피쳐 맵의 업샘플링과 피쳐 채널 수를 절반으로 줄이는 2 x 2 컨볼루션(\"업컨볼루션\"), 축소 경로에서 해당하는 잘린 피쳐 맵과의 연결로 구성됩니다. 2개의 3 x 3 컨볼루션과 각각 ReLU가 뒤따릅니다. 모든 컨볼루션에서 경계 픽셀이 손실되기 때문에 자르기가 필요합니다.\n","\n","<!-- so that the expanding block can resize the input from the contracting block can have the same size as the input from the previous layer -->\n","\n","*Fun fact: 이 아키텍처를 기반으로 하는 이후 모델은 종종 컨볼루션에서 패딩을 사용하여 이미지 크기가 업샘플링/다운샘플링 단계 외부에서 변경되는 것을 방지합니다!*\n","\n","<details>\n","<summary>\n","<font size=\"3\" color=\"green\">\n","<b>Optional hint for <code><font size=\"4\">ExpandingBlock</font></code></b>\n","</font>\n","</summary>\n","\n","1. 연결은 채널 수가 다시 input_channels로 돌아가는 것을 의미하므로 다음 convolution을 위해 다시 반으로 줄여야 합니다.\n","</details>"]},{"cell_type":"code","metadata":{"id":"oM0mKJah4dIs"},"source":["# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: crop\n","def crop(image, new_shape):\n","    '''\n","    Function for cropping an image tensor: Given an image tensor and the new shape,\n","    crops to the center pixels.\n","    Parameters:\n","        image: image tensor of shape (batch size, channels, height, width)\n","        new_shape: a torch.Size object with the shape you want x to have\n","    '''\n","    # There are many ways to implement this crop function, but it's what allows\n","    # the skip connection to function as intended with two differently sized images!\n","    #### START CODE HERE (~7lines)####\n","\n","    \n","    \n","    \n","    \n","    \n","    \n","    #### END CODE HERE ####\n","    return cropped_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TtgdrZp-QSl"},"source":["#UNIT TEST\n","def test_expanding_block_crop(test_samples=100, test_channels=10, test_size=100):\n","    # Make sure that the crop function is the right shape\n","    skip_con_x = torch.randn(test_samples, test_channels, test_size + 6, test_size + 6)\n","    x = torch.randn(test_samples, test_channels, test_size, test_size)\n","    cropped = crop(skip_con_x, x.shape)\n","    assert tuple(cropped.shape) == (test_samples, test_channels, test_size, test_size)\n","\n","    # Make sure that the crop function takes the right area\n","    test_meshgrid = torch.meshgrid([torch.arange(0, test_size), torch.arange(0, test_size)])\n","    test_meshgrid = test_meshgrid[0] + test_meshgrid[1]\n","    test_meshgrid = test_meshgrid[None, None, :, :].float()\n","    cropped = crop(test_meshgrid, torch.Size([1, 1, test_size // 2, test_size // 2]))\n","    assert cropped.max() == (test_size - 1) * 2 - test_size // 2\n","    assert cropped.min() == test_size // 2\n","    assert cropped.mean() == test_size - 1\n","\n","    test_meshgrid = torch.meshgrid([torch.arange(0, test_size), torch.arange(0, test_size)])\n","    test_meshgrid = test_meshgrid[0] + test_meshgrid[1]\n","    crop_size = 5\n","    test_meshgrid = test_meshgrid[None, None, :, :].float()\n","    cropped = crop(test_meshgrid, torch.Size([1, 1, crop_size, crop_size]))\n","    assert cropped.max() <= (test_size + crop_size - 1) and cropped.max() >= test_size - 1\n","    assert cropped.min() >= (test_size - crop_size - 1) and cropped.min() <= test_size - 1\n","    assert abs(cropped.mean() - test_size) <= 2\n","\n","test_expanding_block_crop()\n","print(\"Success!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jufVtUmM-QSl"},"source":["# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED CLASS: ExpandingBlock\n","class ExpandingBlock(nn.Module):\n","    '''\n","    ExpandingBlock Class\n","    Performs an upsampling, a convolution, a concatenation of its two inputs,\n","    followed by two more convolutions.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels):\n","        super(ExpandingBlock, self).__init__()\n","        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        # \"followed by a 2x2 convolution that halves the number of feature channels\"\n","        # \"a concatenation with the correspondingly cropped feature map from the contracting path\"\n","        # \"and two 3x3 convolutions\"\n","        #### START CODE HERE (~3 lines)####\n","\n","        \n","        \n","        #### END CODE HERE ####\n","        self.activation = nn.ReLU() # \"each followed by a ReLU\"\n"," \n","    def forward(self, x, skip_con_x):\n","        '''\n","        Function for completing a forward pass of ExpandingBlock: \n","        Given an image tensor, completes an expanding block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","            skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n","                    for the skip connection\n","        '''\n","        x = self.upsample(x)\n","        x = self.conv1(x)\n","        skip_con_x = crop(skip_con_x, x.shape)\n","        x = torch.cat([x, skip_con_x], axis=1)\n","        x = self.conv2(x)\n","        x = self.activation(x)\n","        x = self.conv3(x)\n","        x = self.activation(x)\n","        return x\n","    \n","    # Required for grading\n","    def get_self(self):\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlZYSY11WbiG"},"source":["#UNIT TEST\n","def test_expanding_block(test_samples=100, test_channels=10, test_size=50):\n","    test_block = ExpandingBlock(test_channels)\n","    skip_con_x = torch.randn(test_samples, test_channels // 2, test_size * 2 + 6, test_size * 2 + 6)\n","    x = torch.randn(test_samples, test_channels, test_size, test_size)\n","    x = test_block.upsample(x)\n","    x = test_block.conv1(x)\n","    # Make sure that the first convolution produces the right shape\n","    assert tuple(x.shape) == (test_samples, test_channels // 2,  test_size * 2 - 1, test_size * 2 - 1)\n","    orginal_x = crop(skip_con_x, x.shape)\n","    x = torch.cat([x, orginal_x], axis=1)\n","    x = test_block.conv2(x)\n","    # Make sure that the second convolution produces the right shape\n","    assert tuple(x.shape) == (test_samples, test_channels // 2,  test_size * 2 - 3, test_size * 2 - 3)\n","    x = test_block.conv3(x)\n","    # Make sure that the final convolution produces the right shape\n","    assert tuple(x.shape) == (test_samples, test_channels // 2,  test_size * 2 - 5, test_size * 2 - 5)\n","    x = test_block.activation(x)\n","\n","test_expanding_block()\n","print(\"Success!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FS_ABXD4dpT"},"source":["## 최종 레이어\n","이제 최종 특징 매핑 블록을 작성합니다. 이 블록은 임의의 많은 텐서가 있는 텐서를 받아 픽셀 수는 같지만 정확한 수의 출력 채널을 가진 텐서를 생성합니다. 논문(Renneberger, 2015) 에서:\n","\n",">최종 레이어에서 1x1 컨볼루션을 사용하여 각 64개 구성 요소 특징 벡터를 원하는 클래스 수에 매핑합니다. 네트워크에는 총 23개의 컨볼루션 레이어가 있습니다.\n"]},{"cell_type":"code","metadata":{"id":"GXCDIjai-0C5"},"source":["# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED CLASS: FeatureMapBlock\n","class FeatureMapBlock(nn.Module):\n","    '''\n","    FeatureMapBlock Class\n","    The final layer of a UNet - \n","    maps each pixel to a pixel with the correct number of output dimensions\n","    using a 1x1 convolution.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels, output_channels):\n","        super(FeatureMapBlock, self).__init__()\n","        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n","        #### START CODE HERE (~1 line)####\n","\n","        #### END CODE HERE ####\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of FeatureMapBlock: \n","        Given an image tensor, returns it mapped to the desired number of channels.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.conv(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDkrByFGjkdE"},"source":["# UNIT TEST\n","assert tuple(FeatureMapBlock(10, 60)(torch.randn(1, 10, 10, 10)).shape) == (1, 60, 10, 10)\n","print(\"Success!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EeFmtgvD__eA"},"source":["## U-Net\n","\n","이제 모두 함께 합칠 수 있습니다! 여기에서 구현한 세 가지 종류의 블록을 결합하는 `UNet` 클래스를 작성합니다."]},{"cell_type":"code","metadata":{"id":"Y8iyZqtVABPM"},"source":["# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED CLASS: UNet\n","class UNet(nn.Module):\n","    '''\n","    UNet Class\n","    A series of 4 contracting blocks followed by 4 expanding blocks to \n","    transform an input image into the corresponding paired image, with an upfeature\n","    layer at the start and a downfeature layer at the end\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels, output_channels, hidden_channels=64):\n","        super(UNet, self).__init__()\n","        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n","        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","        self.contract1 = ContractingBlock(hidden_channels)\n","        self.contract2 = ContractingBlock(hidden_channels * 2)\n","        self.contract3 = ContractingBlock(hidden_channels * 4)\n","        self.contract4 = ContractingBlock(hidden_channels * 8)\n","        self.expand1 = ExpandingBlock(hidden_channels * 16)\n","        self.expand2 = ExpandingBlock(hidden_channels * 8)\n","        self.expand3 = ExpandingBlock(hidden_channels * 4)\n","        self.expand4 = ExpandingBlock(hidden_channels * 2)\n","        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of UNet: \n","        Given an image tensor, passes it through U-Net and returns the output.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        # Keep in mind that the expand function takes two inputs, \n","        # both with the same number of channels. \n","        #### START CODE HERE (~10 lines)####\n","\n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        #### END CODE HERE ####\n","        return xn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdL5DE90jMEU"},"source":["#UNIT TEST\n","test_unet = UNet(1, 3)\n","assert tuple(test_unet(torch.randn(1, 1, 256, 256)).shape) == (1, 3, 117, 117)\n","print(\"Success!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRk_8azSq3tF"},"source":["## 훈련\n","\n","마침내, 당신은 이것을 행동으로 옮기게 될 것입니다!\n","매개변수는 다음과 같습니다.\n","   * criterion: 손실 함수\n","   * n_epochs: 훈련 시 전체 데이터 세트를 반복하는 횟수\n","   * input_dim: 입력 이미지의 채널 수\n","   * label_dim: 출력 이미지의 채널 수\n","   * display_step: 이미지를 표시/시각화하는 빈도\n","   * batch_size: 정방향/역방향 패스당 이미지 수\n","   * lr: 학습률\n","   * initial_shape: 입력 이미지의 크기(픽셀 단위)\n","   * target_shape: 출력 이미지의 크기(픽셀 단위)\n","   * device: 장치 유형\n","\n","훈련하는 데 몇 분 밖에 걸리지 않습니다!\n"]},{"cell_type":"code","metadata":{"id":"UXptQZcwrBrq"},"source":["import torch.nn.functional as F\n","criterion = nn.BCEWithLogitsLoss()\n","n_epochs = 100\n","input_dim = 1\n","label_dim = 1\n","display_step = 100\n","batch_size = 4\n","lr = 0.0002\n","initial_shape = 512\n","target_shape = 373\n","device = 'cuda'\n","#device = 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNAK2XqMJ419"},"source":["from skimage import io\n","import numpy as np\n","\n","volumes = torch.Tensor(io.imread('train-volume.tif'))[:, None, :, :] / 255\n","labels = torch.Tensor(io.imread('train-labels.tif', plugin=\"tifffile\"))[:, None, :, :] / 255\n","labels = crop(labels, torch.Size([len(labels), 1, target_shape, target_shape]))\n","dataset = torch.utils.data.TensorDataset(volumes, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fy6UBV60HtnY"},"source":["def train():\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=True)\n","    unet = UNet(input_dim, label_dim).to(device)\n","    unet_opt = torch.optim.Adam(unet.parameters(), lr=lr)\n","    cur_step = 0\n","\n","    for epoch in range(n_epochs):\n","        for real, labels in tqdm(dataloader):\n","            cur_batch_size = len(real)\n","            # Flatten the image\n","            real = real.to(device)\n","            labels = labels.to(device)\n","\n","            ### Update U-Net ###\n","            unet_opt.zero_grad()\n","            pred = unet(real)\n","            unet_loss = criterion(pred, labels)\n","            unet_loss.backward()\n","            unet_opt.step()\n","\n","            if cur_step % display_step == 0 or display_step == 800:\n","                print(f\"Epoch {epoch}: Step {cur_step}: U-Net loss: {unet_loss.item()}\")\n","                show_tensor_images(\n","                    crop(real, torch.Size([len(real), 1, target_shape, target_shape])), \n","                    size=(input_dim, target_shape, target_shape)\n","                )\n","                show_tensor_images(labels, size=(label_dim, target_shape, target_shape))\n","                show_tensor_images(torch.sigmoid(pred), size=(label_dim, target_shape, target_shape))\n","            cur_step += 1\n","\n","train()"],"execution_count":null,"outputs":[]}]}