{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-practice",
   "metadata": {},
   "source": [
    "## Image-to-Image Translation with Pix2Pix\n",
    "\n",
    "### Image-to-Image Translation\n",
    "이 비디오에서는 이미지-대-이미지 변환에 익숙해지고, 실제 이미지와 다른 유형의 입력에 대해서 어떻게 동작하는지 확인할 수 있습니다. 먼저 이미지-대-이미지 변환이 무엇인지 배우고, 다양한 유형의 이미지-대-이미지 변환과 다른 유형의 변환 작업에 대한 몇 가지 예를 볼 수 있습니다.\n",
    "\n",
    "* 이미지-대-이미지 변환\n",
    "* 다른 유형의 변환들\n",
    "<br>\n",
    "\n",
    "첫 번째 단계로, 이미지-대-이미지 변환이란 무엇입니까? 이미지의 스타일을 다르게 하는 것과 같이 이미지를 가져와서 변환하여 다른 이미지를 얻는다고 상상해 보십시오. 여기 이 **흑백 이미지가 컬러 이미지로 변환**되는 것을 볼 수 있습니다. 어떤 면에서 이것은 실제로 **일종의 조건적 생성**이지만, **한 이미지의 내용을 조건화하는 것**입니다. 예를 들어 이 흑백 이미지를 사용하여 다른 이미지를 생성합니다. 즉, 이 흑백 이미지를 조건화하여 해당 이미지의 컬러 스타일이 제공됩니다.\n",
    "<img src=\"./images/C3W2.02.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "이미지-이미지 변환 작업의 또 다른 예는 **분할맵**으로부터 진행하는 것입니다. 이들은 도로, 자동차 또는 보행자, 보도, 나무의 분할입니다. 그 분할맵으로부터 나무 레이블이 붙은 곳에 나무를, 도로 레이블이 지정된 곳에 도로를, 자동차 레이블이 지정된 곳에 자동차를 **사실적인 사진으로 매핑**합니다. 한 도메인에서 다른 도메인으로 이동하게 됩니다. 시맨틱 분할맵을 사용하여 매우 다르게 보이는 사실적인 뷰를 얻을 수 있다고 상상할 수 있으며, 여기 이 하나의 사실적인 이미지, 이 사진과만 일치할 필요는 없습니다.\n",
    "<img src=\"./images/C3W2.03.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "또 다른 멋진 이미지-대-이미지 변환 작업으로 **비디오에서 비디오로 변환**이 있습니다. 여기에서 비디오 프레임이 다른 비디오의 프레임에 매핑되도록 합니다. 본질적으로 이미지-대-이미지 변환이지만, 많은 이미지 또는 많은 프레임에 적용하는 겁니다. 이 옛날의 흑백 기차 필름을 가져와서 사실적인 색상을 갖는 4K로 만들 수도 있습니다. 이것은 실제로 사실적인 색상을 추가하는 것 뿐만 아니라, 저해상도에서 고해상도(초고해상도라고도 함)로 이동하는 것입니다. 이미지-대-이미지 변환은 GAN이 여기 비디오에서 이러한 매우 사실적인 이미지를 생성할 수 있기 때문에 실제로 빛날 수 있었읍니다. 모든 단일 프레임은 여전히 매우 사실적으로 보입니다.\n",
    "<img src=\"./images/C3W2.04.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "이제 우리가 먼저 탐구할 이미지-대-이미지 변환 유형에 대해 자세히 알아보겠습니다. 첫 번째는 **쌍으로 된 이미지-대-이미지 변환**입니다. 이것이 의미하는 것은 **입력과 출력을 페어링**한다는 것입니다. 이는 훈련 데이터 세트의 경우, 모든 입력 예제에 해당하는 출력 이미지 또는 해당 입력 이미지의 다른 스타일의 내용이 포함된 대상 이미지가 있음을 의미합니다. 그래서 일대일로 매핑됩니다. 기본적으로 당신이 하는 일은 **출력 이미지를 얻기 위해 입력을 조건화하는 것**입니다. 쌍으로 주어진 각각의 두 번째 출력 이미지, 이 건물의 정면과 컬러 나비 이미지에서 알 수 있듯이, 그 분할 맵인 입력에서 생성할 수 있는 유일한 유형의 이미지는 아닙니다. 이러한 레이블에서 생성할 수 있는 다양한 건물과 흑백 이미지에서 생성할 수 있는 다양한 나비 유형을 상상할 수 있습니다. 따라서 **쌍을 이루는 출력 이미지는 반드시 그 ground truth가 아니라 하나의 ground truth, 즉 하나의 가능성**입니다.\n",
    "<img src=\"./images/C3W2.05.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "다음은 **낮에서 밤 시간 사진으로**, 또는 **가장자리 윤곽에서 사진으로** 가는 다른 예입니다. 일반적인 컴퓨터 비전 알고리즘인 가장자리 감지 알고리즘을 사용하여 실제로 가장자리를 얻어 이 쌍을 이루는 훈련 데이터를 생성할 수 있습니다. 이 페어링된 데이터 세트를 쉽게 만들 수 있기 때문에 매우 좋습니다. 그런 다음 훈련된 후에는 실제로 그리는 모든 종류의 가장자리 윤곽에서 사실적인 사진으로 다시 변환할 수 있습니다.\n",
    "<img src=\"./images/C3W2.06.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그리고 이미지-대-이미지 변환이 실제로 훨씬 더 복잡해질 수 있습니다. 단일 입력 이미지를 조건화하는 대신 **다른 옷을 입은 모델**과 **서 있어야 할 위치, 포즈에 대한 포인트별 맵**을 입력으로 사용할 수 있습니다. 그 점들은 다른 포즈를 나타냅니다. 그런 다음 **이 두 가지 정보에서 다른 포즈로 그 사람을 생성**하기를 원합니다. 그것이 바로 여기 오른쪽에 보이는 것입니다.\n",
    "<img src=\"./images/C3W2.07.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "이미지뿐만 아니라 다른 유형의 변환 작업도 분명히 있습니다. 이미지-대-이미지 변환은 실제로 이 매핑이 작동하는 방식을 이해하고 구축할 수 있는 프레임워크라고 생각합니다. 실제로 다른 형식에서도 작동할 수 있습니다. 예를 들어 **텍스트에서 이미지로 이동**할 수 있습니다. 여기에 텍스트가 표시됩니다. \"이 새는 흰색과 붉은색은 띄며, 매우 짧은 부리를 가지고 있습니다\"이라고 쓰여 있습니다. 해당 텍스트를 보고 해당 사진으로 정확히 생성하면 다음과 같아야 합니다.\n",
    "<img src=\"./images/C3W2.08.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "또 다른 멋진 응용 프로그램은 \"말하는 얼굴 신경망\" 또는 \"말하는 이미지\"로서,  먼로의 이미지를 찍은 다음, 나 또는 당신과 같은 다른 사람의 얼굴 벡터를 가져 와서 실제로 먼로를 통해 말할 수 있습니다. 이것들은 실제로 먼로가 말하는 것이 아닙니다. 이들은 다른 사람들이 얼굴 움직임을 만들고 그 **얼굴 움직임, 얼굴 벡터 또는 얼굴 랜드마크를 이 Monroe 이미지에 조건으로 지정**합니다. 이 Monroe 이미지에 애니메이션을 적용할 수 있으며 Einstein 또는 Mona Lisa에게도 이 작업을 수행할 수 있습니다.\n",
    "<img src=\"./images/C3W2.09.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "요약하면 **이미지-대-이미지 변환은 이미지를 다양한 스타일로 변환하는 조건적 생성 프레임워크**입니다. 이미지를 가져와 다른 스타일의 이미지를 얻기 위해 변환하지만 해당 내용은 유지합니다. GAN은 사실적인 생성에 매우 능숙하기 때문에 이 이미지에서 이미지로 변환 작업에 매우 적합합니다. 다음 몇 비디오에서 이러한 GAN 모델 중 일부에 대해 자세히 알아볼 것입니다. 물론 텍스트를 이미지로 또는 이미지를 비디오로 또는 이미지와 얼굴 마크를 비디오로 변환하는 것과 같은 **다른 유형의 변환 작업**도 있습니다. 이것들은 확실히 탐색할 수 있는 것들이며, 이미지에서 이미지로의 변환 프레임워크를 이해하면 이 모든 것을 이해할 수 있습니다.\n",
    "\n",
    "* 이미지-대-이미지 변환은 이미지들을 다양한 스타일로 변환한다.\n",
    "* GAN은 사실적인 생성에 매우 능숙하기 때문에 이미지-대-이미지 변환 작업에 매우 적합하다.\n",
    "* 텍스트-to-이미지 또는 이미지-to-비디오와 같은 다른 유형의 변환 작업도 가능하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c722480",
   "metadata": {},
   "source": [
    "### Pix2Pix Overview\n",
    "이 비디오에서는 쌍으로 된 이미지 변환을 수행하는 조건적 GAN의 한 유형인 Pix2Pix에 대해 배웁니다. 먼저 Pix2Pix를 살펴보고, 그것의 독특한 생성기 및 판별기 설계에 대한 개요를 살펴보는 것으로 시작하겠습니다.\n",
    "\n",
    "* Pix2Pix 개요\n",
    "* 조건적 GAN과의 비교\n",
    "* 개선된 생성자와 판별자의 설계\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Pix2Pix for Paired Image-to-Image Translation\n",
    "\n",
    "먼저 이름을 어떻게 지었는지 살펴보겠습니다. Image-to-Image 변환은 이미지에서 이미지로 진행되며, 이미지를 Pix라 하여, \"Pix-to-Pix\"로, \"to\" 는 숫자 2로 표시됩니다. 이것은 UC Berkeley의 논문으로, 쌍을 이루는 이미지-대-이미지 변환을 수행하기 위해 일종의 조건적 GAN을 매우 성공적으로 사용했습니다. 여기에서는 입력 이미지에 조건을 지정하고 직접적 출력 쌍을 갖도록 하였습니다. \n",
    "<img src=\"./images/C3W2.12.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "먼저, 해당 클래스의 이미지를 생성하기 위해 클래스에 대한 클래스 벡터를 가져오는 생성기인 조건적 GAN을 기억하십시오. 여기에서 Collie는 전달된 one-hot-vector 요소입니다. Collie를 출력으로 생성하는 것을 알고 있는 생성기입니다. 물론 이 클래스 벡터는 이 출력이 실제로 Collie가 되도록 하기 위해 판별자에도 제공됩니다.\n",
    "<img src=\"./images/C3W2.13.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "Pix2Pix도 비슷하지만 **클래스 벡터 대신 실제로 이미지 전체를 입력**으로 전달합니다. 이 이미지는 건물의 분할 맵이라고 할 수 있습니다. 이것이 실제 입력입니다. 이러한 건물을 분할하면 중간 파란색 사각형이 창문이며, 창문 상단의 일부 유형, 발코니는 녹색이고, 전반적인 정면은 이 짙은 파란색입니다. 다른 색상은 해당 이미지 내의 다른 클래스에 해당합니다. 여러분이 볼 수 있듯이 여기 **노이즈 벡터가 지워져 있습니다**. 이는 실제로 논문 저자들이 노이즈 벡터가 생성자의 출력에 큰 차이를 만들지 않는다는 것을 발견했기 때문입니다. 일반적으로 노이즈 벡터가 사용된 목적은 생성자가 이러한 모든 다른 출력을 생성할 수 있도록 하는 것이었습니다. 그러나 그들이 발견한 것은 노이즈 벡터가 실제로 생성된 출력이 어떻게 생겼는지에 있어서 그렇게 큰 차이를 만들지 않는다는 것입니다. 이것은 이 생성자가 얻고자 하는 짝을 이루는 출력 이미지가 있다는 사실 때문일 수 있으며, 이는 곧 보게 될 것입니다. 노이즈 대신에 그들은 실제로 드롭아웃을 사용하여 네트워크에 약간의 확률성을 추가할 수 있음을 발견했습니다. 이는 나중에 보게 될 것입니다. Dropout은 학습할 때 신경망의 특정 레이어에 있는 노드를 무작위로 제거하여, 다른 노드가 다른 것을 배울 수 있도록 하는 것입니다. 결과적으로 이것은 훈련 중 출력에 약간의 임의성을 추가하지만 이전에 입력과 다른 노이즈 벡터를 입력할 때 다양한 개를 만드는것에서 본 것처럼 과감하지 않습니다. 구체적으로 이 예에서 **입력은 건물의 분할 마스크**이고 **출력은 실제 건물**입니다.\n",
    "<img src=\"./images/C3W2.14.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "이제 주어진 생성자 설정 하에서, 판별자가 어떻게 생겼는지 살펴보겠습니다. **판별자도 이 조건으로 생성자에 입력된 분할 맵을 가져옵니다**. 그런 다음 **대상 출력 중 하나와 연결**됩니다. 이것은 분할 마스크를 만들어낸 진짜 건물 또는 생성된 출력입니다. 이것은 생성자가 생성한 것이며, 실제 입력인 분할 마스크는 생성자가 조건화한 분할 마스크이기도 합니다. 분할 마스크는 그것과 연결된 것들 중 하나를 붙여 입력으로 사용되고, 판별자는 그것이 진짜인지 가짜인지를 결정해야 합니다. 이전에 본 조건적 GAN과 매우 유사한 조건을 보게 됩니다. 콜리임을 지정하기 위해서 클래스 벡터를 사용한 것 같이, 여기에서는 전체 분할 마스크를 사용함을 볼 수 있습니다. \"그의 이미지가 실제로 그 분할 마스크의 사실적인 매핑처럼 보입니까?\" 판별기가 여기서 알아내려고 하는 것입니다. Collie의 클래스 벡터가 진짜 또는 가짜인 다양한 Collie 이미지들과 연결될 수 있다는 점과는 다르게, **이 시나리오에서는 해당하는 진짜 출력이 하나만 있습니다**. 이 이미지에 대해 가지고 있는 해당 ground truth는 단 하나뿐입니다. 다시 말하지만, 이것이 한 쌍의 출력 이미지가 있는 이 패러다임에서 저자가 무작위 노이즈를 추가하는 것이 모델 학습과 함께 실제로 변경되지 않는다는 것을 발견한 이유입니다.\n",
    "<img src=\"./images/C3W2.15.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "여기서 꽤 흥미로운 일이 발생합니다. 생성자와 판별자는 모두 업그레이드되며, **생성자는  U-net** 이라고 하는 것을 사용합니다. U-net은 일반적으로 분할에 사용됩니다. 이것은 인코더 유형이므로 이미지를 인코딩한 다음, 디코더에 전달합니다. 그리고, 그 사이에 일부 건너뛰기 연결이 뒤따르는 것을 볼 수 있습니다. 다음 비디오에서 이에 대해 좀 더 자세히 설명하겠습니다. 그리고 **판별자는 Patch GAN**이 됩니다. 이것이 의미하는 바는 이미지의 여러 다른 부분에 대해 더 많은 피드백을 제공한다는 것입니다. 전체 이미지에 대해 진짜인지 가짜인지 말하는 대신 해당 이미지의 여러 다른 패치들에 대해 이것이 진짜인지 가짜인지 판별하게 됩니다. 많은 진짜/가짜 결정이 행렬로 출력됩니다. 이것은 판별자가 생성자에게 더 많은 피드백을 제공할 것임을 의미합니다. 물론 목표는 여전히 이 GAN 훈련과정에서  현실적인 결과를 생성하는 것입니다.\n",
    "<img src=\"./images/C3W2.16.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "요약하면, Pix2Pix의 입력과 출력은 조건적 GAN과 유사합니다. 단, 지금은 클래스 벡터 대신 전체 이미지를 가져오고 해당 이미지가 대상 출력과 일대일로 쌍을 이룬다는 점만 다릅니다. 또한 입력으로 명시적 노이즈가 없습니다. 마지막으로 생성기 및 판별기 모델이 크게 개선되었습니다. 다음 몇 비디오에서 이러한 각 구성요소를 살펴보게 될 것입니다.\n",
    "<img src=\"./images/C3W2.17.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a393846",
   "metadata": {},
   "source": [
    "### Pix2Pix: PatchGAN\n",
    "가장 먼저 배울 구성 요소는 **PatchGAN이라는 Pix2Pix 판별자**입니다. 따라서 이 비디오에서 PatchGAN 아키텍처가 무엇인지에 대한 개요를 얻을 수 있습니다. 이는 주로 단일 값이 아닌, **행렬을 출력**하는 것에 관한 것입니다.\n",
    "\n",
    "* PatchGAN 판별자의 구조\n",
    "* 행렬 출력 대 단일 값 출력\n",
    "<br>\n",
    "\n",
    "따라서 PatchGAN은 단일 출력 대신 분류 매트릭스를 출력합니다. 여기에서 이미지의 패치를 보고 있는 것을 알 수 있으며, 전체 행렬에서 하나의 값을 넣는 것입니다. 이 행렬의 각 값은 여전히 0과 1 사이에 있으며 여기서 0은 가짜이고 1은 진짜입니다. \n",
    "<img src=\"./images/C3W2.19.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "따라서 이전과 여전히 동일하며 PatchGAN은 모든 70 x 70 패치에 대해 이 작업을 수행합니다. 이 다른 패치는 행렬의 이 다른 출력 값에 해당합니다. \n",
    "<img src=\"./images/C3W2.19-1.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "입력 이미지의 모든 패치에 걸쳐 시야를 슬라이드하면 PatchGAN이 이미지의 각 영역 또는 패치에 대한 피드백을 제공합니다. 그리고 각 패치가 실제일 확률을 출력하기 때문에 여전히 BCE 손실로 훈련할 수 있습니다. 따라서 **생성기의 가짜 이미지의 경우 이것이 의미하는 바는 PatchGAN이 모두 0으로 구성된 행렬을 출력하려고 시도**해야 한다는 것입니다. 그래서 그것에 대한 레이블, 여기에 해당하는 레이블은 모두 0으로 구성된 이 행렬입니다. 예, 이 이미지의 모든 패치는 가짜입니다. \n",
    "<img src=\"./images/C3W2.20.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그리고 데이터 세트의 **진짜 이미지에 대해서**도 동일한 논리가 적용되므로 patch는 실제로 이미지의 **각 패치가 진짜임을 나타내는 모든 행렬의 출력을 시도**할 수 있습니다.\n",
    "<img src=\"./images/C3W2.21.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "따라서 Pix2Pix에 대해 요약하면 판별자는 진짜 또는 가짜의 단일 값 대신 값의 행렬을 출력합니다. 여기서 0은 여전히 가짜 분류에 해당하고 1은 여전히 실제 분류에 해당합니다.\n",
    "<img src=\"./images/C3W2.22.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc93fb",
   "metadata": {},
   "source": [
    "### Pix2Pix: U-Net\n",
    "Pix2Pix의 다음 구성 요소는 **U-Net을 기반으로 하는 업그레이드된 생성기**입니다. 먼저 U-Net 아키텍처 프레임워크가 어떻게 생겼는지 살펴보겠습니다. 그 사이에서 **건너뛰기 연결을 사용하는 인코더-디코더 모델**이기도 합니다. 그러면 이것이 Pix2Pix 생성기로 어떻게 작동하는지 알 수 있습니다.\n",
    "\n",
    "* U-Net 프레임워크\n",
    "  - 엔코더(encoder)-디코더(decoder)\n",
    "  - 건너뛰기 연결(Skip connection)\n",
    "  \n",
    "* Pix2Pix 생성기\n",
    "<br>\n",
    "\n",
    "U-Net은 일반적으로 이미지 분할을 위한 매우 성공적인 컴퓨터 비전 모델이었습니다. **분할은 실제 이미지를 가져와 이미지의 모든 픽셀에 대해 분할 마스크 또는 레이블을 할당**하는 것입니다. 그래서 당신은 자동차들에 라벨을 붙이고, 횡단보도에 라벨을 붙이고, 나무에 라벨을 붙이고, 도로에 라벨을 붙이고, 보행자에게 라벨을 붙입니다. 자율주행차에 많이 활용되는 기능입니다. 그리고 **분할 작업**은 이미지를 이미지로 변환하는 작업이며, 각 픽셀이 무엇인지, **각 픽셀이 속한 클래스에 대한 정답이 있습니다**. 따라서 자동차의 픽셀은 확실히 자동차가 될 것이며 다른 방식으로 분할할 수 없습니다. \n",
    "그러나 Pix2Pix가 하는 것처럼, **무언가를 생성하는 데서는 정답이 없습니다**. 여기 이 자동차의 이미지는 그 차가 어떤것이어야 하는지에 대한 정답이 없습니다. 제 말은, 저 이미지에서 이 차가 있을 수 있지만, 그것은 단지 분할 마스크이기 때문에 실제로는 다른 자동차일 수 있습니다. 이 모든 자동차가 인간으로 본다면 기술적으로 옳지 않습니까? 따라서 이것은 이미지에서 이미지로의 변환 작업이기도 합니다. 이제 주목해야 할 중요한 점은 **Pix2Pix가 생성기 아키텍처에 U-Net 아키텍처를 사용한다는 것**입니다. **입력 이미지를 가져와서 출력 이미지에 매핑하는 데 능숙**하기 때문입니다. 그리고 일반적으로 이미지 분할에만 사용되지만 Pix2Pix는 이 생성 작업에도 사용하려고 합니다. 그리고 Pix2Pix가 하는 일은 대체로 이 둘 사이를 오갈 수 있다는 것입니다.\n",
    "<img src=\"./images/C3W2.24.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "기존의 생성기 아키텍처는 작은 노이즈 벡터를 사용한다는 점을 기억하십시오. U-Net 생성기는 실제로 전체 이미지를 가져오므로 여기에 $x$가 있습니다. 따라서 이것은 생성기가 이미지 입력을 처리하기 위해 컨볼루션으로 훨씬 더 강력해야 함을 의미합니다. 따라서 **Pix2Pix 생성기의 아키텍처는 이 인코더-디코더 구조**로, 먼저 사물을 인코딩합니다. 인코더는 분류 모델에 가깝다고 상상할 수 있습니다. 이미지를 가져와서 고양이 또는 개 클래스에 대한 값을 여기에 출력할 수 있기 때문입니다. 그리고 이 중간 레이어를 이 이미지에 정보를 포함할 수 있는 병목으로 생각할 수도 있습니다. 그래서 그 이미지에 있는 모든 중요한 정보는 이 작은 공간에 압축됩니다. 그래서 당신은 그 높은 수준의 특징, 중요한 특징을 얻고 그것을 디코딩하여 또 다른 이미지인 y를 출력할 수 있습니다. 그리고 이것은 자동 인코더(AutoEncoder)를 생각나게 할 수 있습니다. 하지만 AutoEncoder와는 달리 x에 가능한 한 유사하고 싶지 않을 것입니다. 여기서 여러분은 그것을 원하지 않습니다. y가 x를 조건으로 하는 다른 스타일이 되기를 원합니다.\n",
    "<img src=\"./images/C3W2.25.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그러나 이러한 네트워크를 훈련 이미지 쌍에 과적합하기 쉽기 때문에 **U-Net은 인코더에서 디코더로 건너뛰기 연결(skip connection)도 도입**하며 이는 또한 매우 유용합니다. 인코딩 단계에서 손실되었을 수 있는 정보를 얻습니다. 따라서 인코딩 단계에서 디코딩 단계의 해당 블록과 동일한 해상도의 모든 단일 블록이 이 추가 연결을 통해 해당 값과 연결됩니다. **너무 많이 압축되었을 수 있는 정보가 계속 흘러나와 이러한 나중 계층 중 일부에 계속 도달**할 수 있습니다. 따라서 이러한 건너뛰기 연결은 디코더의 각 컨볼루션 블록으로 이동하기 전에 인코더에서 연결됩니다. 그리고 건너뛰기 연결은 컨볼루션 신경망(CNN)에서 꽤 표준적이며 대부분 정보가 이전 레이어에서 이후 레이어로 흐르도록 허용합니다. 이 정보에서 추가되거나 연결될 수 있지만 어떻게든 이후 계층에 포함되며 여기에서 연결됩니다. 따라서 **인코더가 디코더로 다운샘플링하는 과정에서 잃어버렸을 수 있는 특정 세부 정보를 쉽게 얻을 수 있습니다**. 그리고 이것은 주로 정보의 흐름에 관한 것입니다. 물론 이것은 순방향 전파 과정입니다.\n",
    "<img src=\"./images/C3W2.26.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "**역방향 패스**에서 건너뛰기 연결은 역방향으로 갈 때 이 **그라디언트 흐름을 개선**할 수도 있습니다. 따라서 너무 많은 레이어를 함께 쌓을 때의 그라디언트 소멸 문제를 돕기 위해 건너뛰기 연결이 도입되었습니다. 그라디언트 소멸은 그라디언트가 역전파에서 곱해질 때 너무 작아져서 네트워크가 더 깊어지고 더 많은 레이어를 갖는 것을 제한합니다. 역방향 패스에서 U-Net의 멋진 점은 인코더에 대한 그래디언트 흐름을 개선하여 해당 레이어가 여기 디코더에 있었을 수 있는 정보에서도 학습할 수 있다는 것입니다. \n",
    "<img src=\"./images/C3W2.27.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "먼저 너비와 높이가 256 x 256 이고 3개의 색상 채널이 있는 RGB인 이미지 $x$를 사용하는 인코더가 있습니다. 그리고 이 분할된 이미지가 조건화된 이미지로 입력되고 이 모든 블록을 통과합니다.\n",
    "<img src=\"./images/C3W2.28.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "8개의 인코더 블록을 거쳐 입력을 압축합니다. 그리고 각 블록은 2배만큼 공간 크기로 다운샘플링합니다. 따라서 인코더 맨 끝의 출력 공간 크기는 256을 2로 8번 나눈 값 또는  1 x 1 크기이며 해당 정보를 인코딩하기 위한 512개의 채널이 있습니다. 따라서 맨 끝에는 높이 너비가 1 x 1이고, 각 인코더 블록에는 컨볼루션 층, 배치정규화 층 및 LeakyReLU 활성화가 포함됩니다.  StyleGAN과 유사하게 진행되는 이러한 블록의 레이어라는 것을 여러 번 보았기 때문에 이것은 당신에게 큰 놀라움이 아닐 수도 있습니다. 따라서 컨볼루션은 보폭이 2인 높이와 너비를 사용하여 입력을 더 작게 만듭니다. 따라서 지금까지 보아온 많은 컨볼루션이 1의 보폭을 가짐에 반해서, 이들은 2의 보폭을 가집니다.\n",
    "<img src=\"./images/C3W2.29.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "한편, 디코더 측에서는 1 x 1의 입력 크기, 이 작고 작은 입력을 가지고 있으며 다시 8개의 블록이 있습니다. 하지만 이들은 디코더 블록입니다. 그리고 인코더 입력과 동일한 크기의 출력 이미지를 생성하기를 원하기 때문에 디코더는 실제로 인코더와 동일한 수의 블록을 포함합니다. 그런 다음 생성된 이미지인 출력으로 y를 얻습니다. 이는 입력 256 x 256 x 3 색상 채널과 동일한 크기입니다. 놀랍지도 않은 각 디코더 블록은 전치된 컨볼루션으로 구성됩니다. 그래서 입력을 받고 출력을 더 크게 만들고 BatchNorm과 ReLU 활성화 함수가 뒤따릅니다. \n",
    "Dropout이 이 네트워크에 추가되었지만 실제로는 이 디코더의 처음 세 블록에 추가되었습니다. Dropout은 각 반복 학습에서 무작위로 뉴런을 비활성화하여 다른 뉴런이 학습할 수 있도록 합니다. 이는 동일한 뉴런이 항상 같은 것을 학습하지 않도록 하기 위한 것입니다. 그리고 이 불확실성인 이 잡음이 네트워크에 추가됩니다. 이것은 훈련 중에만 존재하며 Dropout의 모든 사용과 마찬가지로 일반적으로 추론 또는 테스트 시간 동안에는 꺼집니다. 그리고 그 추론 뉴런은 실제로 그들이 기대하는 것과 같은 종류의 분포를 유지하기 위해 이 inverse Dropout 확률에 의해 조정됩니다. 이것들이 그다지 중요하지 않지만 Dropout이 이 모델에 일종의 노이즈를 추가한다는 점만 알아두십시오. 우리는 지금 입력으로 노이즈를 제거하고 있으므로, 이것이 이 모델 아키텍처에 확률론이 스며드는 곳이지만 훈련 중에만 가능하다는 것을 기억하십시오. 추론하는 동안에는 물론 그런 유형의 확률을 볼 수 없으며, 그런 유형의 노이즈를 주입할 수도 없습니다.\n",
    "<img src=\"./images/C3W2.30.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그리고 두 반쪽을 합치면 여기에서 완전한 인코더-디코더 구조를 얻을 수 있습니다. 여기서 인코더는 256 x 256의 입력을 받아 동일한 크기의 출력인 생성된 이미지를 출력합니다. 입력에 대한 정보는 1 x 1 크기의 작은 병목을 통해 전달되며 이 작은 공간 크기는 해당 입력 이미지의 인코딩을 요약하는 것으로 이해될 수 있습니다. 그런 다음 디코더는 인코더의 역연산을 수행하는 것으로 생각할 수 있습니다. 이것이 동일한 수의 블록 또는 8개의 블록을 포함하는 이유입니다.\n",
    "<img src=\"./images/C3W2.31.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "따라서 U-Net 프레임워크는 인코더-디코더 프레임워크의 변형으로, 모든 단일 인코더 블록 또는 레벨의 인코딩이 동일한 해상도의 디코더의 해당 블록 레벨로 전달된다는 점을 기억하십시오. 그리고 U-Net은 연결을 사용하여 인코더에서 디코더로 이 정보를 통합합니다. 따라서 인코더 출력을 해당 블록 수준의 각 디코더 입력에 연결하면 동일한 해상도이므로 쉽게 연결할 수 있습니다.\n",
    "<img src=\"./images/C3W2.32.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "요약하면 Pix2Pix는 생성기에 U-Net를 사용하고, U-Net은 동일한 해상도 또는 동일한 블록 또는 동일한 레벨을 연결하는 건너뛰기 연결을 사용하는 인코더-디코더 프레임워크입니다. 특징들은 인코더에서 디코더로 서로 매핑되며, 이는 인코딩 단계에서 손실되는 세부 사항이 있는 경우 디코더가 인코더에서 직접 더 자세한 내용을 학습하는 데 도움이 됩니다. 그리고 건너뛰기 연결은 역방향 레이어에서도 도움이 되며, 물론 디코더에서 인코더로 더 많은 그라디언트가 흐르도록 돕습니다.\n",
    "<img src=\"./images/C3W2.33.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77dc994",
   "metadata": {},
   "source": [
    "### Pix2Pix : Pixel Distance Loss Term\n",
    "Pix2Pix의 마지막 구성 요소는 **픽셀 거리 손실 항**입니다. 따라서 먼저 정규화와 손실 항을 추가하는 것이 무엇을 의미하는지 검토한 다음, 쌍을 이루는 **생성된 출력과 진짜 출력 사이의 픽셀 거리**를 적게하려고 노력하는 Pix2Pix에서 사용되는 이 정확한 픽셀 거리 손실 항에 대해 이야기하겠습니다. 이것은 Pix2Pix 생성기에 대한 추가 손실 항입니다.\n",
    "\n",
    "* 정규화와 추가 손실 항\n",
    "* 생성된 출력과 실제 출력 간의 픽셀 거리 계산\n",
    "* Pix2Pix 생성기를 위한 추가 손실 항\n",
    "<br>\n",
    "\n",
    "손실을 **적대적 손실**로 공식화할 수 있다는 점을 기억하십시오. 이는 아마도 **BCE 손실 또는 Wasserstein-손실**일 수 있읍니다. 그리고  **L1 정규화 또는 그래디언트 페널티**와 같은 마지막 항을 추가하고, 이 **Lambda 항을 추가하여 해당 손실 함수를 일정량(일반적으로 1 미만)만큼 가중**할 수 있습니다. 당신의 주요 손실 함수인 적대적 손실을 압도하지 않도록 하기 위함입니다. 적대적 손실은 GAN 손실에 또 다른 이름일 뿐입니다. \n",
    "<img src=\"./images/C3W2.35.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "특히 Pix2Pix의 경우 **출력물이 보기 좋게 보이도록** 하려면 여기에 실제로 **픽셀 손실 항을 추가**하여 생성기에 실제 대상 이미지에 대한 정보를 조금 더 제공할 수 있으므로 더 가깝게 일치시킬 수 있습니다.\n",
    "<img src=\"./images/C3W2.36.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그럼 여기서 의미하는 바를 알아보자. 픽셀 거리를 위해서 마지막 항에 생성기에서 생성된 출력과 실제 목표 출력을 넣습니다. 이것은 여기에서 실제 건물이 생성된 것일 뿐이며 생성된 출력이 실제 출력에 최대한 가깝게 되도록 하기 위해 둘 사이의 픽셀 차이를 가져옵니다. 이 때문에 픽셀 거리가 정말 작으면 이미지가 거의 정확히 동일하다는 것을 의미하며, 크기가 정말 크면 이미지가 실제로 멀리 떨어져 있고 생성된 이미지가 실제 대상 출력과 정말 멀다는 것을 의미합니다. 따라서 픽셀 거리 손실은 이러한 사실성을 돕는 데 매우 유용합니다. 이로써 실제에 정확히 가까워지게 할 겁니다. 그러나 여기에는 까다로운 것이 있습니다. 이것은 \"**지도학습**\"을 추가한 것이며, **생성자가 이제 진짜 이미지를 볼 수 있게 된 겁니다**. 그렇죠? 단지 조건을 위한 실제 입력 이미지를 볼 뿐만 아닙니다. 이것은 이전에 본 핫 클래스 요소와 비슷하지만 이제는 이것이 매우 부드러운 방법으로 주로 해당 샘플을 매우 좋아 보이게 하지만, **어떤 방식으로든 쌍으로 주어진 출력을 암시적으로 봅니다**.\n",
    "<img src=\"./images/C3W2.37.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "종합하면 총 Pix2Pix 생성기는 BCE 손실로 구성됩니다. 적대적 손실에 해당하는 BCE 손실에 여기에 표시되는 픽셀 거리 손실을 더한 것입니다.\n",
    "<img src=\"./images/C3W2.38.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그리고 **더 수식적으로** 이것은 여러 다른 샘플들을 살펴보기, 곱하기 학습하거나 조정할 수 있는 일종의 가중치 Lambda 와 생성된 출력과 진짜 간의 차이와 특히 이들 간의 $L1$ 또는 픽셀 거리를 살펴봅니다.\n",
    "<img src=\"./images/C3W2.39.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "요약하자면 Pix2Pix는 생성자 손실함수에 가짜 출력에서 실제 목표 출력 사이의 픽셀 차이를 취하는 손실 함수(L1 정규화 항)을 추가하였다고 볼 수 있습니다. 그리고 이것은 생성기가 이미지를 실제와 유사하게 만들도록 장려하고 출력이 실제 이미지를 보다 직접적으로 반영하기 때문에 이미지에서 이미지로의 변환에 괜찮습니다. 그리고 전반적으로 이 마지막 항은 생성된 이미지가 실제 대상과 유사하도록 권장하며, 이러한 추가 지도 레이어는 이 스타일 변환 작업에 확실히 도움이 됩니다.\n",
    "<img src=\"./images/C3W2.40.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8bef0",
   "metadata": {},
   "source": [
    "### Pix2Pix : Putting It all Together\n",
    "이제 Pix2Pix의 모든 구성 요소를 결합할 시간입니다. 여기에는 U-Net 생성기, patchGAN 판별기 및 픽셀 거리 손실 항이 포함됩니다.\n",
    "<img src=\"./images/C3W2.41.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "예를 들어 분할한 실제 건물의 데이터 세트가 있습니다. 한 쌍의 실제 이미지 출력과 함께 건물의 이러한 모든 훌륭한 분할 마스크가 있습니다. 분할 마스크에서 생성할 수 있는 사실적인 건물이나 해당 분할 마스크의 모든 특징과 일치하는 건물을 다시 원합니다. \n",
    "훈련을 마친 후에는 이 분할 마스크처럼 자신만의 마스크를 그릴 수 있고, 그러면 GAN으로 실제 건물을 생성할 수 있습니다. 이는 매우 멋진 일입니다. 이것은 귀하의 입력입니다. U-Net 생성기에 넣으면 일부 출력이 생성됩니다. 그런 다음 이미지는 채널 차원을 따라 원래의 실제 이미지, 즉 컨디셔닝에 사용된 실제 입력 이미지에서 연결됩니다. 이것은 patchGAN 판별기인 판별기로 들어가며, 여러 값들로 채워진 행렬, 0과 1 사이의 분류 행렬을 출력하여 이미지의 여러 부분들이 얼마나 실제인지 또는 가짜인지를 나타냅니다.\n",
    "<img src=\"./images/C3W2.42.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그런 다음 판별자의 손실에 대해 생성된 출력을 실제 입력과 연결한 출력을 가짜 레이블 행렬과 비교합니다. 이 행렬은 모두 0인 행렬입니다. 왜냐하면 판별자는 해당 이미지의 모든 단일 패치를 가짜로 분류하면 성공하기 때문입니다.\n",
    "<img src=\"./images/C3W2.43.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그런 다음 진짜 출력에서, 즉, 진짜 출력이 있다고 가정하면 판별자는 자신의 예측에서 이 분류 행렬의 모든 항목에 가능한 한 근접하기를 원하기 때문에 분류 행렬을 실제 레이블잉 1들과 비교하려고 합니다. 다시 이 분할맵이 입력으로 채널 차원을 따라 연결되었습니다. \n",
    "<img src=\"./images/C3W2.43-1.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "이제 **생성기의 손실**에서는 채널 차원을 따라 실제 입력과 연결된 생성된 출력을 보는 것은 여전히 판별자입니다. 그러나 생성기는 판별자가 생성된 이미지의 모든 단일 패치가 실제처럼 보인다고 생각하기를 원하기 때문에 모두 0인 행렬 대신 1인 실제 레이블이 지정된 행렬에 대한 손실을 보게됩니다. 그러나 그것은 생성기의 손실은 이것 만이 아닙니다. 그것은 이 두 행렬 사이에 BCE 손실을 사용하는 것입니다. 생성기에는 또한 생성된 출력이 실제 목표 출력과 얼마나 다른지 살펴보는 이 픽셀 거리 손실과 가중치인 Lambda를 곱한  항이 있습니다. 그것은 전체 생성기 손실입니다.\n",
    "<img src=\"./images/C3W2.44.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "요약하면 Pix2Pix는 이미지와 이미지 사이를 변환하는 생성기에 U-Net을 사용하고, 단일 값이 아닌 값의 행렬을 출력하는 patchGAN 판별기를 사용하며, 생성기 손실에는 추가 픽셀 거리 손실 항이 있습니다. 이는 더 실제적인 것을 생성하고 쌍을 이루는 훈련 데이터 세트에 있는 목표 출력을 활용하는 데 도움이 됩니다.\n",
    "<img src=\"./images/C3W2.45.png\" width=\"700\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27f31f",
   "metadata": {},
   "source": [
    "### Pix2Pix Advancements\n",
    "Pix2Pix 이후에 무슨 일이 일어났는지 궁금할 것입니다. 많은 다양한 개선 사항이 있습니다. 쌍으로 된 이미지-대-이미지 변환을 위한 Pix2Pix를 확장한 일부 개선 사항을 볼 수 있습니다. 여기에는 **고해상도 이미지**가 포함되며, 매우 명확한 애플리케이션으로서 **이미지 편집**도 있습니다.\n",
    "<img src=\"./images/C3W2.46.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "먼저 버클리의 같은 그룹과 NVIDIA에서 더 높은 해상도의 이미지에서 작동하는 또 다른 모델이 나왔으며 **Pix2PixHD** 이라고 합니다. Pix2PixHD의 정말 멋진 점은 훨씬 **더 높은 해상도의 이미지**에서 작동하고, 훨씬 **더 나은 많은 수정을 포함**한다는 것입니다. 따라서 이미지를 이미지로 변환하고 싶다면 Pix2PixHD 페이퍼와 노트북(옵션)을 확인하는 것이 좋습니다. 그리고 여기에 표시된 것은 일반 Pix2Pix에서도 사용할 수 있는 매우 멋진 응용입니다. 기본적으로 당신이 할 수 있는 일은 누군가의 얼굴의 분할 마스크를 가질 수 있고, 그 마스크를 원하는 대로 조정할 수 있으며, 다른 유형의 얼굴을 생성할 수 있다는 것입니다. 그리고 실제로 여기에서 하고 있는 일은 마스크를 바꾸지 않는 것입니다. 실제로 이 하나의 마스크에서 생성할 수 있는 다양한 가능한 얼굴들이 있습니다. \n",
    "<img src=\"./images/C3W2.47.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "그리고 나온 또 다른 정말 멋진 모델은 **NVIDIA의 GauGAN**입니다. 그리고 이것은 Paul Gauguin이라는 예술가에 대한 말장난입니다. 그리고 GauGAN은 응용 프로그램 측면에서 매우 훌륭합니다. 나는 그들이 컨퍼런스에서 발표한 데모를 아직도 기억합니다. 기본적으로 하는 일은 여기 왼쪽에 **스케치를 그려서 어떤 종류의 클래스인지 표시**할 수 있습니다. 그래서 당신은, 나는 여기에 하늘을 원하고, 나는 이 한 줄의 물을 원한다고 하면, 이 응용은 당신을 위해 이 **사실적인 사진을 생성**할 수 있습니다. 그래서 제 예술 수준은 확실히 이 왼쪽에 있다고 생각합니다. 그래서 생성된 오른쪽에 있는 것이 저에게는 매우 멋집니다. GauGAN의 멋진 점은 **StyleGAN에서 본 몇 가지 고급 기능을 사용**한다는 것입니다. 그리고 실제로 **적응형 인스턴스 정규화**를 사용하여 분할 맵을 가져오고 AdaIN을 사용하여 스타일을 다시 알려주는 적응형 인스턴스 정규화를 사용합니다.\n",
    "<img src=\"./images/C3W2.48.png\" width=\"700\">\n",
    "<br>\n",
    "\n",
    "요약하자면 **Pix2PixHD와 GauGAN은 Pix2Pix의 후속 제품**입니다. 그리고 그들은 Pix2Pix를 기반으로 하여 고해상도 이미지에서 놀라운 발전을 이루었습니다. 그래서 여러분들이 그들을 확인하는 것이 좋겠습니다.\n",
    "<img src=\"./images/C3W2.49.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
